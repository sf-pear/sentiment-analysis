{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Twitter data\n",
    "\n",
    "Project by Sabrina Fonseca Pereira, Ida Maria Zachariassen, Magnus Sverdrup, Rasmus Bondo Hansen and Ruben Oliver Jonsman\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project presented in this notebook was developed with the purpose of predicting the intention or state of mind (pragmatics) of social media data from Twitter.\n",
    "\n",
    "The data was provided from the [TweetEval](https://github.com/cardiffnlp/tweeteval/) corpus, a collection of 7 datasets for different classification tasks. Each task had test, train and validation datafiles consisting of one tweet per line with corresponding labelling in a separate file. Given a tweet we were to predict a label, based on a model trained on tokens of our data.\n",
    "\n",
    "More precisely this project aims to predict the label of the binary classification task **Irony** and the multi class classification task **Stance**, specifically **Atheism**.\n",
    "The **Irony** tweets to either be labelled as <code>0 - non_irony</code> or <code>1 - irony</code> and the **Atheism** tweets to either be labelled as <code>0 - none</code>, <code>1 - against</code> or <code>2 - favor</code>.\n",
    "For further testing the possibility of predicting stance, all topics in the *Stance* task were gathered to see if it was possible to detect both topic and stance, getting a total of 15 labels.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections as cl\n",
    "from collections import Counter\n",
    "import difflib as dl\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import nltk\n",
    "import nltk.tokenize\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import flatten, padded_everygram_pipeline, pad_both_ends\n",
    "from nltk.lm import MLE\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, accuracy_score, classification_report, roc_auc_score, roc_curve, recall_score, precision_score, f1_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phraser, Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting theme for all plots\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS = {}\n",
    "\n",
    "#Path to the irony datasets\n",
    "PATHS[\"irony\"] = \"./datasets/irony/{}.txt\"\n",
    "\n",
    "#Path to the stance datasets\n",
    "PATHS[\"stance\"] = \"./datasets/stance/{}.txt\"\n",
    "\n",
    "#Path to the combined stance dataset\n",
    "PATHS[\"combined_stance\"] = \"./processed/stance/{}\"\n",
    "\n",
    "#Path to our manual annotation answers\n",
    "PATHS[\"MAA\"] = \"./processed/{}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_irony = pd.read_csv(PATHS[\"irony\"].format(\"mapping\"), delimiter = \"\\t\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_irony = pd.read_csv(PATHS[\"irony\"].format(\"train_text\"), delimiter = \"\\0\", header = None, names = [\"tweets\"])\n",
    "train_irony_labels = pd.read_csv(PATHS[\"irony\"].format(\"train_labels\"), delimiter = \"\\0\", header = None, names=[\"labels\"])\n",
    "test_irony = pd.read_csv(PATHS[\"irony\"].format(\"test_text\"), delimiter = \"\\0\", header = None, names = [\"tweets\"])\n",
    "test_irony_labels = pd.read_csv(PATHS[\"irony\"].format(\"test_labels\"), delimiter = \"\\0\", header = None, names=[\"labels\"])\n",
    "validation_irony = pd.read_csv(PATHS[\"irony\"].format(\"val_text\"), delimiter = \"\\0\", header = None, names = [\"tweets\"])\n",
    "val_irony_labels = pd.read_csv(PATHS[\"irony\"].format(\"val_labels\"), delimiter = \"\\0\", header = None, names=[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split irony training dataset into two subsets. One for creating our tokenizers and one for evaluating them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tokenizer = train_irony[len(train_irony)-101:len(train_irony)-1]\n",
    "train_irony = train_irony[0:len(train_irony)-100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_stance = pd.read_csv(PATHS[\"stance\"].format(\"mapping\"), delimiter = \"\\t\", header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Abortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_abortion = pd.read_csv(PATHS[\"stance\"].format(\"/abortion/train_text\"), delimiter = \"\\0\", header = None, names = [\"tweets\"])\n",
    "train_abortion_labels = pd.read_csv(PATHS[\"stance\"].format(\"/abortion/train_labels\"), delimiter = \"\\0\", header = None, names=[\"labels\"])\n",
    "test_abortion = pd.read_csv(PATHS[\"stance\"].format(\"/abortion/test_text\"), delimiter = \"\\0\", header = None, names = [\"tweets\"])\n",
    "test_abortion_labels = pd.read_csv(PATHS[\"stance\"].format(\"/abortion/test_labels\"), delimiter = \"\\0\", header = None,names=[\"labels\"])\n",
    "validation_abortion = pd.read_csv(PATHS[\"stance\"].format(\"/abortion/val_text\"), delimiter = \"\\0\", header = None, names = [\"tweets\"])\n",
    "validation_abortion_labels = pd.read_csv(PATHS[\"stance\"].format(\"/abortion/val_labels\"), delimiter = \"\\0\", header = None, names=[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Atheism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_atheism = pd.read_csv(PATHS[\"stance\"].format(\"/atheism/train_text\"), delimiter = \"\\0\", header = None, names = [\"tweets\"])\n",
    "train_atheism_labels = pd.read_csv(PATHS[\"stance\"].format(\"/atheism/train_labels\"), delimiter = \"\\0\", header = None, names=[\"labels\"])\n",
    "test_atheism = pd.read_csv(PATHS[\"stance\"].format(\"/atheism/test_text\"), delimiter = \"\\0\", header = None, names = [\"tweets\"])\n",
    "test_atheism_labels = pd.read_csv(PATHS[\"stance\"].format(\"/atheism/test_labels\"), delimiter = \"\\0\", header = None, names=[\"labels\"])\n",
    "validation_atheism = pd.read_csv(PATHS[\"stance\"].format(\"/atheism/val_text\"), delimiter = \"\\0\", header = None, names = [\"tweets\"])\n",
    "validation_atheism_labels = pd.read_csv(PATHS[\"stance\"].format(\"/atheism/val_labels\"), delimiter = \"\\0\", header = None, names=[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_climate = pd.read_csv(PATHS[\"stance\"].format(\"/climate/train_text\"), delimiter = \"\\0\", header = None, names = [\"tweets\"])\n",
    "train_climate_labels = pd.read_csv(PATHS[\"stance\"].format(\"/climate/train_labels\"), delimiter = \"\\0\", header = None, names=[\"labels\"])\n",
    "test_climate = pd.read_csv(PATHS[\"stance\"].format(\"/climate/test_text\"), delimiter = \"\\0\", header = None, names = [\"tweets\"])\n",
    "test_climate_labels = pd.read_csv(PATHS[\"stance\"].format(\"/climate/test_labels\"), delimiter = \"\\0\", header = None, names=[\"labels\"])\n",
    "validation_climate = pd.read_csv(PATHS[\"stance\"].format(\"/climate/val_text\"), delimiter = \"\\0\", header = None, names = [\"tweets\"])\n",
    "validation_climate_labels = pd.read_csv(PATHS[\"stance\"].format(\"/climate/val_labels\"), delimiter = \"\\0\", header = None, names=[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feminist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feminist = pd.read_csv(PATHS[\"stance\"].format(\"/feminist/train_text\"), delimiter = \"\\0\", header = None, names = [\"tweets\"])\n",
    "train_feminist_labels = pd.read_csv(PATHS[\"stance\"].format(\"/feminist/train_labels\"), delimiter = \"\\0\", header = None, names=[\"labels\"])\n",
    "test_feminist = pd.read_csv(PATHS[\"stance\"].format(\"/feminist/test_text\"), delimiter = \"\\0\", header = None, names = [\"tweets\"])\n",
    "test_feminist_labels = pd.read_csv(PATHS[\"stance\"].format(\"/feminist/test_labels\"), delimiter = \"\\0\", header = None, names=[\"labels\"])\n",
    "validation_feminist = pd.read_csv(PATHS[\"stance\"].format(\"/feminist/val_text\"), delimiter = \"\\0\", header = None, names = [\"tweets\"])\n",
    "validation_feminist_labels = pd.read_csv(PATHS[\"stance\"].format(\"/feminist/val_labels\"), delimiter = \"\\0\", header = None, names=[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hillary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hillary = pd.read_csv(PATHS[\"stance\"].format(\"/hillary/train_text\"), delimiter = \"\\0\", header = None, names = [\"tweets\"])\n",
    "train_hillary_labels = pd.read_csv(PATHS[\"stance\"].format(\"/hillary/train_labels\"), delimiter = \"\\0\", header = None, names=[\"labels\"])\n",
    "test_hillary = pd.read_csv(PATHS[\"stance\"].format(\"/hillary/test_text\"), delimiter = \"\\0\", header = None, names = [\"tweets\"])\n",
    "test_hillary_labels = pd.read_csv(PATHS[\"stance\"].format(\"/hillary/test_labels\"), delimiter = \"\\0\", header = None, names=[\"labels\"])\n",
    "validation_hillary = pd.read_csv(PATHS[\"stance\"].format(\"/hillary/val_text\"), delimiter = \"\\0\", header = None, names = [\"tweets\"])\n",
    "validation_hillary_labels = pd.read_csv(PATHS[\"stance\"].format(\"/hillary/val_labels\"), delimiter = \"\\0\", header = None, names=[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual annotation answers for the irony dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAA = pd.read_csv(PATHS[\"MAA\"].format(\"manual_annotations\"))\n",
    "MAA = MAA.drop('Unnamed: 0', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the task of splitting a string of characters into minimal processed units, also called tokens, which will be the input in our machine learning solutions.\n",
    "As a starting point we aimed at segmenting the lines at “words” and turned to discuss the significance of written language on social media platforms like Twitter. \n",
    "\n",
    "We saw that our ideal tokenizer should:\n",
    "* Keep words\n",
    "* Remove emojis\n",
    "* Keep numbers\n",
    "* Make everything lower case\n",
    "\n",
    "Further interesting tokenizers were:\n",
    "* Only #\n",
    "* Only Non-words\n",
    "* Only Uppercase letters\n",
    "* Only emojis\n",
    "\n",
    "We did this using the RegEx module. \n",
    "\n",
    "We lastly compared the output of our final tokenizer with a baseline [tweet tokenizer](https://www.nltk.org/api/nltk.tokenize.html) from the NLTK library. To investigate this we used the difflib library. Here each token is compared and the difference between them displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ideal(line):\n",
    "    \"\"\"\n",
    "    Takes a string as an input, this is a line or a tweet from our database. Splits the relevant words or characters into\n",
    "    separate tokens.\n",
    "    Returns either the tokens as a whitespace delimited string or as a list of the tokens. Also returns the non tokens as a list\n",
    "    \n",
    "    Keeps words, ?, !, ..., +, - and turns & into \"and\"\n",
    "    removes dots, commas, # and %\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    unmatchables = []\n",
    "    \n",
    "    for word in line.split():\n",
    "        if re.findall(r\"\\w+-\\w+|\\w+'\\w+|\\w+|[&?!…]+\", word) != []:\n",
    "            x = re.findall(r\"\\w+-\\w+|\\w+'\\w+|\\w+|[&?!…]+\", word)\n",
    "            for element in x:\n",
    "                if element == \"âž\" or element == \"ðŸ\":\n",
    "                    continue\n",
    "                elif element == \"&\":\n",
    "                    tokens.append(\"and\")\n",
    "                else:\n",
    "                    tokens.append(element.lower())\n",
    "\n",
    "        if re.findall(r\"\\w+-\\w+|\\w+'\\w+|\\w+|[&?!…]\", word) != [word] and re.findall(r\"[^\\w|&!?…]+\", word) != []:\n",
    "            unmatchables.append(re.findall(r\"[^\\w|!?…&]+\", word)[0])\n",
    "\n",
    "    s = \" \".join(tokens)\n",
    "    return (s, unmatchables, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ekstra(line):\n",
    "    \"\"\"\n",
    "    Works same way as tokenize_ideal\n",
    "    \n",
    "    Keeps words, dots, ?, %, !, #, ..., +, - and turns & into \"and\"\n",
    "    Removes emojies and commas\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    unmatchables = []\n",
    "    \n",
    "    for word in line.split():\n",
    "        if re.findall(r\"\\w+-\\w+|\\w+'\\w+|\\w+|[.&?%!#…]+\", word) != []:\n",
    "            x = re.findall(r\"\\w+-\\w+|\\w+'\\w+|\\w+|[.&?%!#…]+\", word)\n",
    "            for element in x:\n",
    "                if element == \"âž\" or element == \"ðŸ\":\n",
    "                    continue\n",
    "                elif element == \"&\":\n",
    "                    tokens.append(\"and\")\n",
    "                else:\n",
    "                    tokens.append(element.lower())\n",
    "\n",
    "        if re.findall(r\"\\w+-\\w+|\\w+'\\w+|\\w+|[.&?%!#…]\", word) != [word] and re.findall(r\"[^\\w|.&!?%#…]+\", word) != []:\n",
    "            unmatchables.append(re.findall(r\"[^\\w|.!#?%…&]+\", word)[0])\n",
    "\n",
    "    s = \" \".join(tokens)\n",
    "    return (s, unmatchables, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_caps(data):\n",
    "    \"\"\"\n",
    "    Finds words that only consist of uppercase letters\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = []\n",
    "    unmatchable = []\n",
    "\n",
    "    line = data[:500]\n",
    "    #print(line)\n",
    "    skip_pat = re.compile(r'\\s+')\n",
    "    token_pat = re.compile(r'\\b[A-Z]+\\b')\n",
    "\n",
    "    while line:\n",
    "\n",
    "        skip_match = re.search(skip_pat, line)\n",
    "        if skip_match and skip_match.start() == 0:\n",
    "            line = line[skip_match.end():]\n",
    "        else:\n",
    "            token_match = re.search(token_pat, line)\n",
    "            if token_match and token_match.start() == 0:\n",
    "                tokens.append(line[:token_match.end()])\n",
    "                line = line[token_match.end():]\n",
    "            else:\n",
    "                unmatch_end = len(line)\n",
    "                if skip_match:\n",
    "                    unmatch_end = skip_match.start()\n",
    "                if token_match:\n",
    "                    unmatch_end = min(unmatch_end, token_match.start())\n",
    "\n",
    "                unmatchable.append(line[:unmatch_end])\n",
    "                line = line[unmatch_end:]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonword_only_tokenizer(listofstrings):\n",
    "    \"\"\"\n",
    "    Keeps tokens that are not normal words\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    unmatchables = []\n",
    "\n",
    "    for string in listofstrings:\n",
    "        tokens.append(re.findall(r'[^a-zA-Z\\s]', string))\n",
    "        unmatchables.append(re.findall(r'[\\w]+', string))\n",
    "\n",
    "    return (tokens, unmatchables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_only_tokenizer(listofstrings):\n",
    "    \"\"\"\n",
    "    Keeps only the hastagged words\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    unmatchables = []\n",
    "\n",
    "    for string in listofstrings:\n",
    "        hashtags = re.findall(r'#(\\w+)', string)\n",
    "        tokens.append(hashtags)\n",
    "\n",
    "        not_hashtags = []\n",
    "        all_words = re.findall(r'\\w+', string)\n",
    "        for word in all_words:\n",
    "            if word not in hashtags:\n",
    "                not_hashtags.append(word)\n",
    "        unmatchables.append(not_hashtags)\n",
    "\n",
    "    return (tokens, unmatchables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOJI = \"[\\U00010000-\\U0010ffff]\"\n",
    "NOT_EMOJI = r'[^\\U00010000-\\U0010ffff]+'\n",
    "EMOJICON = r\"(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)\"\n",
    "# EMOJICON-regex found on: https://stackoverflow.com/questions/28077049/regex-matching-emoticons\n",
    "\n",
    "def emoji_cons(tweet):\n",
    "    \"\"\"\n",
    "    Keeps tokens that are emojis or emojicons\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    unmatchables = []\n",
    "\n",
    "    for string in tweet:\n",
    "        if re.findall(EMOJI, string) != []:\n",
    "            x = re.findall(EMOJI, string)\n",
    "            for element in x:\n",
    "                tokens.append(re.findall(EMOJI, string))\n",
    "        elif re.findall(EMOJICON, string) != []:\n",
    "\n",
    "            tokens.append(re.findall(EMOJICON, string))\n",
    "    if re.findall(EMOJICON, tweet) != []:\n",
    "        tokens.append(re.findall(EMOJICON, tweet))\n",
    "\n",
    "    if re.findall(NOT_EMOJI, tweet) != [] and re.findall(NOT_EMOJI, tweet) not in tokens:\n",
    "        unmatchables.append(re.findall(NOT_EMOJI, tweet))\n",
    "\n",
    "    return (tokens, unmatchables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a few different tokenizers. However, to illsutrate our work we will through most of the notebook stick with our \"tokenize_ideal\" tokenizer. In the end when we have created our ML models to predict stance and irony, we will compare the different tokenizers and evaluate which one is the best. <br>\n",
    "<br>\n",
    "Here is a small sample when using our \"tokenize_ideal\" function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "tokens_list = []\n",
    "tokens_string = []\n",
    "unmatches = []\n",
    "for tweet in list(eval_tokenizer[\"tweets\"]):\n",
    "    tokenized = tokenize_ideal(tweet)\n",
    "    tokens_list.append(tokenized[2])\n",
    "    tokens_string.append(tokenized[0])\n",
    "    for token in tokenized[2]:\n",
    "        tokens.append(token)\n",
    "    for un in tokenized[1]:\n",
    "        unmatches.append(un)\n",
    "\n",
    "print(tokens_list[0:5])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with baseline tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how general tokenizers work, and to figure out the quality of our own tokenizers, we wanted to compare our tokenizers with a baseline tokenisation tool. Here we use the social media tokeniser called TweetTokenizer from the nltk library: https://www.nltk.org/api/nltk.tokenize.html. Since we removed a small part from the training dataset in the begnining, we now have some tweets to evaluate our tokenizers and compare our tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk_tokens = []\n",
    "nltk_list = []\n",
    "for tweet in list(eval_tokenizer[\"tweets\"]):\n",
    "    tokenized = tknzr.tokenize(tweet)\n",
    "    nltk_list.append(tokenized)\n",
    "    for token in tokenized:\n",
    "        nltk_tokens.append(token)\n",
    "print(nltk_list[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the difference quantitatively we can use the method SequenceMatcher from the difflib library: https://docs.python.org/3/library/difflib.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_ratio = []\n",
    "for i in range(len(tokens_list)):\n",
    "    match_ratio.append(dl.SequenceMatcher(None, nltk_list[i], tokens_list[i]).ratio())\n",
    "print(\"Average ratio:\",np.mean(match_ratio))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that on average our tokenizer and the NLTK baseline tokenizer agree 65% of the time. However, none of these two methods are perfect, and this number of 65% is not really good/bad or high/low. Alternativly we can also look at the actual difference between the two tokenizers.<br>\n",
    "<br>\n",
    "Here we again use the difflib library, now a method called unified_diff, which returns the area where there differences occur. As an example we only look at the first tweet. <br>\n",
    "<code>\"+\"</code> means that the word is present in the output from the baseline tokenizer but not in the output from our tokenizer. <br>\n",
    "<code>\"-\"</code> means that the word is not present in the output from the baseline tokenizer but it is in our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "for i in range(n):\n",
    "    print(\"Our tokenized tweet:\\0\", tokens_list[i], \"\\0\")\n",
    "    print(\"NLTK tokenized tweed:\\0\", nltk_list[i], \"\\0\")\n",
    "    print(\"Difference between the two:\")\n",
    "    for diff in dl.unified_diff(tokens_list[i], nltk_list[i]):\n",
    "        print(diff, \"\\0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can however be seen that the comparing is case sensitive, which causes the biggest difference, as it was decided to lower case everything in our tokenizer. <br>\n",
    "To summerize, our chosen tokenizer actually removes more than the baseline. In the end we will conclude if this makes a difference when predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Characterising Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function to quickly report the number of lines, words and characters in case unix commands for macs doesn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(class_string, data_string):\n",
    "    \"\"\"Shows the details that wc unix command would show: \n",
    "    amount of lines, words and characters for the stance datasets.\"\"\"\n",
    "    \n",
    "    file = open(\"./datasets/\"+ data_string +\"/train_text.txt\", \"r\", encoding = \"utf-8\")\n",
    "\n",
    "    num_lines = 0; num_words = 0; num_char = 0\n",
    "    for line in file:\n",
    "        line = line.strip(\"\\0\")\n",
    "\n",
    "        words = line.split()\n",
    "        num_lines += 1\n",
    "        num_words += len(words)\n",
    "        num_char += len(line)\n",
    "\n",
    "    print(num_lines, num_words, num_char)\n",
    "    print(\"Number of lines, words and characters in the training text file for the\", class_string, data_string, \"\\0\")\n",
    "\n",
    "    with open(\"./datasets/\"+ data_string +\"/train_labels.txt\", \"r\") as file:\n",
    "        filelist = file.read().split(\"\\0\")\n",
    "\n",
    "    print(len(filelist))\n",
    "    print(\"Number of lines in the training label file for the\", class_string, data_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify our process of finding statistics we create a function to quickly create a vocabulary for a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary(tokens):\n",
    "    vocabulary = pd.DataFrame()\n",
    "    vocabulary_counter = cl.Counter(tokens)\n",
    "    vocabulary[\"word\"] = vocabulary_counter.keys()\n",
    "    vocabulary[\"frequency\"] = vocabulary_counter.values()\n",
    "    vocabulary = vocabulary.sort_values(by = [\"frequency\"], ascending = False)\n",
    "    \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irony\n",
    "Corpus, vocabulary and token-ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unix commands that work for mac\n",
    "!wc \"./datasets/irony/train_text.txt\"\n",
    "print(\"Number of lines, words and characters in the training text file for the binary class Irony. \\0\")\n",
    "\n",
    "!wc -l \"./datasets/irony/train_labels.txt\"\n",
    "print(\"Number of lines in the training label file for the binary class Irony.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the above with distinction that it is viewable with windows os.\n",
    "word_count(\"binary\", \"irony\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_irony = len(train_irony) + len(test_irony) + len(validation_irony)\n",
    "print((len(test_irony)/sum_irony)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count of tweets per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irony_train = pd.concat([train_irony, train_irony_labels], axis=1)\n",
    "irony_val   = pd.concat([validation_irony, val_irony_labels], axis=1)\n",
    "irony_test  = pd.concat([test_atheism,  test_irony_labels],  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irony_train['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irony_val['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irony_test['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token-ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First tokenizing all the tweets in the <i>Irony</i> class using the <code>Tokenize_ideal</code> function, then creating a corresponding vocabulary and the unique token frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irony_tokens = []\n",
    "irony_tokens_list = []\n",
    "irony_unmatches = []\n",
    "\n",
    "for tweet in train_irony[\"tweets\"]:\n",
    "    tokenized = tokenize_ideal(tweet)\n",
    "    irony_tokens_list.append(tokenized[2])\n",
    "    for token in tokenized[2]:\n",
    "        irony_tokens.append(token)\n",
    "    for un in tokenized[1]:\n",
    "        irony_unmatches.append(un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irony_vocabulary = vocabulary(irony_tokens)\n",
    "print(\"Number of tokens: \", sum(irony_vocabulary[\"frequency\"]))\n",
    "print(\"Number of unique tokens: \", len(irony_vocabulary[\"frequency\"]))\n",
    "irony_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most frequent tokens in our vocabulary and their corresponding occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(irony_vocabulary[\"word\"])[0:10])\n",
    "print(list(irony_vocabulary[\"frequency\"])[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amount of least occuring tokens and examples hereof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "    print(\"Amount of tokens occurring\", i,\"time(s):\", len(irony_vocabulary[irony_vocabulary[\"frequency\"] == i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    print(\"5 tokens occuring\", i, \"time(s) \\0\", irony_vocabulary[\"word\"][irony_vocabulary[\"frequency\"] == i][:5], \"\\0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency table\n",
    "Accumulating the frequency for each word in the vocabulary dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = []\n",
    "count = 0\n",
    "for i in range(len(irony_vocabulary[\"frequency\"])):\n",
    "    count = count + list(irony_vocabulary[\"frequency\"])[i]\n",
    "    accu.append(count/len(irony_tokens))\n",
    "    \n",
    "irony_vocabulary[\"cumulative_frequency\"] = accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irony_vocabulary[\"rank\"] = [i+1 for i in range(len(irony_vocabulary[\"word\"]))]\n",
    "irony_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the cummulative count for the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = irony_vocabulary[\"rank\"], y = irony_vocabulary[\"cumulative_frequency\"], linewidth=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Illustration of Zipf's law <br>\n",
    "\"*given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word.*\", this quote is taken from [Wikipedia](https://en.wikipedia.org/wiki/Zipf%27s_law). With this law, we can expect that very few words make up the biggest part of any given context. To verify this claim, one can in a log-log transformed system plot the frequency of each token against the tokens rank. This should create something close to a straight line. <br>\n",
    "<br>\n",
    "Therefore we add two new columns to our vocabulary dataframe, each is the log transformed values for the frequency and the rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irony_vocabulary[\"log_frequency\"] = np.log(irony_vocabulary[\"frequency\"])\n",
    "irony_vocabulary['log_rank'] = np.log(irony_vocabulary[\"rank\"])\n",
    "#vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the log frequency and the log rank we see an almost linear relationship between the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = irony_vocabulary, x = \"log_rank\", y = \"log_frequency\", linewidth=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = irony_vocabulary['frequency'].sum()\n",
    "irony_vocabulary['proportion'] = irony_vocabulary['frequency']/N\n",
    "irony_vocabulary['predicted_proportion'] = (1/irony_vocabulary['rank']**1)/(np.sum(1/(np.arange(1, N+1)**1))) # zipfs prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_irony, a = plt.subplots(figsize = (8,5))\n",
    "a = sns.scatterplot(x=irony_vocabulary[\"rank\"], y=irony_vocabulary['proportion'], linewidth=0)\n",
    "a = sns.scatterplot(x=irony_vocabulary[\"rank\"], y=irony_vocabulary['predicted_proportion'], linewidth=0)\n",
    "a.set(yscale=\"log\", xscale=\"log\")\n",
    "plt.xlabel(\"log(rank of words)\")\n",
    "plt.ylabel(\"log(porpotion of words)\")\n",
    "plt.title(\"Irony: Rank of a word and its empirical and theoretical probability\", size=14)\n",
    "fig_irony.legend(labels=['Empirical', 'Theoretical'], bbox_to_anchor=(.89, .86), fontsize=12)\n",
    "plt.show(fig_irony)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams\n",
    "To investigate the relationship between words, we are going to use n-grams. This method is explained through the Markov assumption: \"*Each element of the sequence depends only on the immediately preceding element and is independent of the previous history*\". With this we can define the k'th order Markov assumption: \"*Each element of the sequence depends only on the k immediately preceding elements.*\" <br>\n",
    "<br>\n",
    "With this tool we can define the probabilities of k preceding words for every unique word in a given context. When we are able to predict words, we are also able to predict meaning or context of new sentences. As mentioned we will be using n-grams in our project which comes from the python library NLTK which has multiple n-gram implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin we are going to look at a bigram. To prepare every tweet for the n-gram model, we have to pad them. Meaning we are putting a \"s\" (start of sentence) symbol at the front, and a \"/s\" (end of sentence) symbol at the end. This can give meaning to words that tend to appear more often in the beginning or at the end. <br>\n",
    "<br>\n",
    "Here we see a \"bigram\", which is just a n-gram with degree of 2. Meaning we are only looking 1 word back in the sentence, for every word. For example the word \"walking\" is in this case being related to the preceding word \"ppl\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2 = list(nltk.bigrams(pad_both_ends(irony_tokens_list[0], n = 2)))\n",
    "print(ex2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a trigram, n-gram with degree of 3. Here we are looking and defining based on the 2 preceding words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex3 = list(nltk.trigrams(pad_both_ends(irony_tokens_list[0], n = 3)))\n",
    "print(ex3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine the bigram and trigram to create everygrams. Like this we also include the lower degrees of grams. If we for example use a everygram with degree of 3, we not only create the trigram, but we also include the bigram and unigram information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_bigrams = list(pad_both_ends(irony_tokens_list[0], n=2))\n",
    "ex_every = list(everygrams(padded_bigrams, max_len=3))\n",
    "print(ex_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the tweets prepared we can begin to train a model. To do this we have created a small function that takes the tokenized tweets and a degree of the everygram. Here we are using the pipeline function from NLTK which does the padding for us very easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ngram(listoflistoftokens, n):\n",
    "    train, vocab = padded_everygram_pipeline(n, listoflistoftokens)\n",
    "    lm = MLE(n)\n",
    "    lm.fit(train, vocab)\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train two models, one with degree 3 and one with degree 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3 = train_ngram(irony_tokens_list, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the models we can look at how confused it is when trying to predict on words. Here we use the NLTK perplexity function, the higher the score the more options the model has to choose from, meaning it is unsure what is most correct. To test this we feed the first sentence in the irony training dataset to the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ex_every:\n",
    "    print(i)\n",
    "    print(lm3.perplexity([i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both models it seems to have an okay (low) perplexity with some of the words, but for others it has a high score, meaning it is very confused about what to predict here. <br>\n",
    "<br>\n",
    "Lastly we can also use these models to generate sentences based on the training data. The generated data is often hard to make sense of or just a copy of a tweet. This is probably due to the model being quite confused, or that there only exists one prediction for this specific case. When talking NLP we have to keep in mind that our training data is a small dataset, and therefore we would expect low performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm3.generate(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum likelihood is about estimating preceding words in a given context. Meaning what is the probability of x given y. This is simply the sum of x given y in the corpus divied by the number of times y appear in the corpus:<br>\n",
    "\n",
    "$$p(w_2|w_1) = \\frac{\\text{count}(w_1w_2)}{\\text{count}(w_1\\bullet)}$$\n",
    "<br>\n",
    "To illustrate the maximum liklihood principle, and the problems with it, we creating a subset of the irony training dataset. These tweets we tokenize, pad and create their bigrams and put into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tokens = []\n",
    "mx_test = list(train_irony[\"tweets\"][0:100])\n",
    "for tweet in mx_test:\n",
    "    tokenized = tokenize_ideal(tweet)\n",
    "    bigram_tokens.append(tokenized[2])\n",
    "padded_bigram_tokens = []\n",
    "for tokens in bigram_tokens:\n",
    "    bi = list(nltk.bigrams(pad_both_ends(tokens, n = 2)))\n",
    "    padded_bigram_tokens.append(bi)\n",
    "flat_tokens = []\n",
    "mm = [[flat_tokens.append(bigram) for bigram in line] for line in padded_bigram_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a dictionary that stores every preceding word to every unique token in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def mx(tokens): (OBS: takes around 1 min to run)\n",
    "uniques = list(irony_vocabulary[\"word\"])\n",
    "probs = {unique:[] for unique in uniques}\n",
    "df = pd.DataFrame(irony_vocabulary[\"word\"])\n",
    "for unique in uniques:\n",
    "    for bigram in flat_tokens:\n",
    "        if bigram[-1] == unique:\n",
    "            probs[unique].append(bigram[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we sum all the times the specific x given y appears and divide by the times y appears in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {key:{} for key in probs.keys()}\n",
    "for key in probs:\n",
    "    x = set(probs[key])\n",
    "    for item in x:\n",
    "        n = 0\n",
    "        u = 0\n",
    "        for i in flat_tokens:\n",
    "            if item == i[0]:\n",
    "                n += 1\n",
    "            if item == i[0] and key == i[1]:\n",
    "                u += 1\n",
    "        dic[key].update({item:u/n})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then see all the probabilities of the preceding words for the token \"user\". For example, the probability of \"user\" given \"user\" is 34.5% percent, and the probability of \"user\" given \"feat\" is 100% percent. Meaning if our model wants try predict what word comes after the token \"feat\", then it will with 100% certainty predict the token \"user\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic[\"user\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for this model to predict \"feat\" as preciding \"user\" with 100% certainty is that \"feat\" only appears once in the corpus. This is obviously not good, since we know there are many more posibilities of words that can appear after \"feat\" than just \"user\" in future tweets. To tackle this problem we will discuss smoothing techniques next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing Techniques\n",
    "Smoothing techniques in NLP are used to address scenarios related to determining probability / likelihood estimate of a sequence of words (say, a sentence) occuring together when one or more words individually (unigram) or N-grams such as bigram(wi/wi−1) or trigram (wi/wi−1wi−2) in the given set have never occured in the past.\n",
    "\n",
    "##### Kneser–Ney smoothing\n",
    "Kneser–Ney smoothing is a method primarily used to calculate the probability distribution of n-grams in a document based on their histories. It is widely considered the most effective method of smoothing due to its use of absolute discounting by subtracting a fixed value from the probability's lower order terms to omit n-grams with lower frequencies.\n",
    "\n",
    "Relying on only the unigram frequency to predict the frequencies of n-grams leads to skewed results; however, Kneser–Ney smoothing corrects this by considering the frequency of the unigram in relation to possible words preceding it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Kneser–Ney smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kneser Ney with nltk - Sabrina \n",
    "# is this the right tokenized dataset to be using: irony_tokens_list?\n",
    "trigrams = [ngrams(sent, 3) for sent in irony_tokens_list]\n",
    "fdist = FreqDist([item for l in trigrams for item in l])\n",
    "kneser_ney = nltk.KneserNeyProbDist(fdist)\n",
    "\n",
    "# KneserNeyProbDist: Kneser-Ney estimate of a probability distribution. This is a version of back-off that counts how likely an n-gram is provided the n-1-gram had been seen in training. Extends the ProbDistI interface, requires a trigram FreqDist instance to train on. Optionally, a different from default discount value can be specified. The default discount is set to 0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fdist = pd.DataFrame.from_dict(fdist, orient='index')\n",
    "df_fdist.columns = ['frequency']\n",
    "df_fdist.index.name = 'n-grams'\n",
    "df_fdist_sorted = df_fdist.sort_values(by=['frequency'], ascending=False).reset_index()\n",
    "df_fdist_sorted['kneser_ney_prob'] = df_fdist_sorted['n-grams'].apply(kneser_ney.prob)\n",
    "df_fdist_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about word embedding was mostly found here: https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/ <br>\n",
    "The code for the word embedding implementation was greatly inspired by this web post: https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every word can be represented by a vector. Word embeddings are N-dimensional vectors that try to capture word-meaning and context in their values. For the vectors to be useful, they should for a vocabulary capture the meaning of the words, the relationship between words, and the context of different words as they are used naturally. <br>\n",
    "<br>\n",
    "To capture this we can encode different meanings to the words, and therefore represent each word as an N dimensional vector. Then the relation between each word is measured by the euclidean distance. <br>\n",
    "<br>\n",
    "If one manages to create meaningfull encodings and assigns the values of each word in a good way, it can help machine learning models. Meaning words that were not seen during training, but is known in the word embedding space, will not be the cause of the model not functioning. For example if a model is trained to recognise vectors for “car”, “van”, “jeep”, or “automobile”, it will still behave well to the vector for “truck” due to the similarity of the vectors. Word embedding only works well with alot of data, thereore we are going to need more data than just the irony training data. For the demonstration of word embedding we are going to use all the training data from \"emotion\", \"hate\", \"irony\", \"offensive\" and \"sentiment\" datasets, as well as the testing data from the \"sentiment\" dataset (because it provided a lot of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emotion = pd.read_csv(\"./datasets/emotion/train_text.txt\", delimiter = \"\\0\", names = [\"tweet\"])\n",
    "train_hate = pd.read_csv(\"./datasets/hate/train_text.txt\", delimiter = \"\\0\", names = [\"tweet\"])\n",
    "train_offensive = pd.read_csv(\"./datasets/offensive/train_text.txt\", delimiter = \"\\0\", names = [\"tweet\"])\n",
    "train_sentiment = pd.read_csv(\"./datasets/sentiment/train_text.txt\", delimiter = \"\\0\", names = [\"tweet\"])\n",
    "train_iron = pd.read_csv(\"./datasets/irony/train_text.txt\", delimiter = \"\\0\", names = [\"tweet\"])\n",
    "test_sentiment = pd.read_csv(\"./datasets/sentiment/test_text.txt\", delimiter = \"\\0\", names = [\"tweet\"])\n",
    "master = pd.concat([train_emotion, train_iron, train_hate, train_offensive, train_sentiment, test_sentiment])\n",
    "len(master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all this data combined we now have 75000 tweets. We then tokenize the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_list = []\n",
    "for tweet in master[\"tweet\"]:\n",
    "    tokenized = tokenize_ideal(tweet)\n",
    "    master_list.append(tokenized[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = master_list\n",
    "common_terms = [\"of\", \"with\", \"without\", \"and\", \"or\", \"the\", \"a\", \"it\", \"it's\", \"its\"]\n",
    "phrases = Phrases(all_sentences, connector_words = common_terms)\n",
    "bigram = Phraser(phrases)\n",
    "all_sentences = list(bigram[all_sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our Word2Vec model with the tokenized master dataset. We set our vectors to have a dimension of 100, set the context window to be 5 words and exclude words that only appear once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = Word2Vec(all_sentences, \n",
    "                 min_count=2,   # Ignore words that appear less than this\n",
    "                 vector_size=100,      # Dimensionality of word embeddings\n",
    "                 workers=2,     # Number of processors (parallelisation)\n",
    "                 window=5,      # Context window for words during training\n",
    "                 epochs=30)       # Number of epochs training over corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model.wv.vectors.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a vocabulary of 35206 unique words, which each appears more than once in the master dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model.wv.most_similar(\"good\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the top words that are supposed to have similar context - often of same lexical category. One would expect the word \"good\" to lie next to words like \"nice\" and \"great\", and indeed our model is predicting this. However, it is also predicting the opposite meaning words like \"bad\" and \"hard\", as they are also words that could replace \"good\" and the sentence would probably still make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stance: Atheism\n",
    "Corpus, vocabulary and token-ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc \"./datasets/stance/atheism/train_text.txt\"\n",
    "print(\"Number of lines, words and characters in the training text file for the multi class Stance: atheism. \\0\")\n",
    "\n",
    "!wc -l \"./datasets/stance/atheism/train_labels.txt\"\n",
    "print(\"Number of lines in the training label file for the multi class Stance: atheism.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the above with distinction that it is viewable with windows os.\n",
    "word_count(\"multiclass\", \"stance/atheism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_atheism = len(train_atheism) + len(test_atheism) + len(validation_atheism)\n",
    "print((len(test_atheism)/sum_atheism)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count of tweets per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atheism_train = pd.concat([train_atheism, train_atheism_labels], axis=1)\n",
    "atheism_val   = pd.concat([validation_atheism, validation_atheism_labels], axis=1)\n",
    "atheism_test  = pd.concat([test_atheism,  test_atheism_labels],  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atheism_train['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atheism_val['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atheism_test['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token-ration\n",
    "Tokenizing the tweets of the <i>Stance: Atheism</i> class and creating the corresponding vocabulary as well as the token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atheism_tokens = []\n",
    "atheism_tokens_list = []\n",
    "atheism_unmatches = []\n",
    "for tweet in train_atheism[\"tweets\"]:\n",
    "    tokenized = tokenize_ideal(tweet)\n",
    "    atheism_tokens_list.append(tokenized[2])\n",
    "    for token in tokenized[2]:\n",
    "        atheism_tokens.append(token)\n",
    "    for un in tokenized[1]:\n",
    "        atheism_unmatches.append(un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atheism_vocabulary = vocabulary(atheism_tokens)\n",
    "print(\"Number of tokens: \", sum(atheism_vocabulary[\"frequency\"]))\n",
    "print(\"Number of unique tokens: \", len(atheism_vocabulary[\"frequency\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most frequent tokens and their corresponding count in the vocabulary for the <i> Stance: Abortion </i> class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(atheism_vocabulary[\"word\"])[0:10])\n",
    "print(list(atheism_vocabulary[\"frequency\"])[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amount of least occuring tokens and examples hereof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "    print(\"Amount of tokens occurring\", i,\"times:\", len(atheism_vocabulary[atheism_vocabulary[\"frequency\"] == i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    print(\"5 tokens occuring\", i, \"times \\0\", atheism_vocabulary[\"word\"][atheism_vocabulary[\"frequency\"] == i][:5], \"\\0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulating the frequency for each word in the vocabulary dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = []\n",
    "count = 0\n",
    "for i in range(len(atheism_vocabulary[\"frequency\"])):\n",
    "    count = count + list(atheism_vocabulary[\"frequency\"])[i]\n",
    "    accu.append(count/len(atheism_tokens))\n",
    "    \n",
    "atheism_vocabulary[\"cumulative_frequency\"] = accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atheism_vocabulary[\"rank\"] = [i+1 for i in range(len(atheism_vocabulary[\"word\"]))]\n",
    "atheism_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the cummulative count for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = atheism_vocabulary[\"rank\"], y = atheism_vocabulary[\"cumulative_frequency\"], linewidth=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Illustration of Zipf's law <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atheism_vocabulary[\"log_frequency\"] = np.log(atheism_vocabulary[\"frequency\"])\n",
    "atheism_vocabulary['log_rank'] = np.log(atheism_vocabulary[\"rank\"])\n",
    "#vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the log frequency and the log rank we see an almost linear relationship between the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = atheism_vocabulary, x = \"log_rank\", y = \"log_frequency\", linewidth=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Na = atheism_vocabulary['frequency'].sum()\n",
    "atheism_vocabulary['proportion'] = atheism_vocabulary['frequency']/Na\n",
    "atheism_vocabulary['predicted_proportion'] = (1/atheism_vocabulary['rank']**1)/(np.sum(1/(np.arange(1, Na+1)**1))) # zipfs prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_atheism, a = plt.subplots(figsize = (8,5))\n",
    "a = sns.scatterplot(x=atheism_vocabulary[\"rank\"], y=atheism_vocabulary['proportion'], linewidth=0)\n",
    "a = sns.scatterplot(x=atheism_vocabulary[\"rank\"], y=atheism_vocabulary['predicted_proportion'], linewidth=0)\n",
    "a.set(yscale=\"log\", xscale=\"log\")\n",
    "plt.xlabel(\"log(rank of words)\")\n",
    "plt.ylabel(\"log(porpotion of words)\")\n",
    "plt.title(\"Atheism: Rank of a word and its empirical and theoretical probability\", size=14)\n",
    "fig_atheism.legend(labels=['Empirical', 'Theoretical'], bbox_to_anchor=(.89, .86), fontsize=12)\n",
    "plt.show(fig_atheism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the datasets\n",
    "Comment on the difference between the binary class and the multi-class  \n",
    "mean words per tweet  \n",
    "most frequent words in each  \n",
    "how many words occur once twice etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First looking at the average number of characters and words in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average character per tweet\n",
    "lengths_atheism = [len(tweet) for tweet in train_atheism[\"tweets\"]]\n",
    "lengths_irony = [len(tweet) for tweet in train_irony[\"tweets\"]]\n",
    "print(\"Average character for atheism dataset:\", round(np.mean(lengths_atheism), 2))\n",
    "print(\"Average character for irony dataset:\", round(np.mean(lengths_irony), 2), \"\\0\")\n",
    "print(\"On Average a tweet from the atheism dataset is using\",  round(np.mean(lengths_atheism)/np.mean(lengths_irony), 2), \n",
    "      \"times more characters than from the irony dataset.\\0\")\n",
    "\n",
    "#Average word per tweet\n",
    "lengths_atheism = [len(tweet.split()) for tweet in train_atheism[\"tweets\"]]\n",
    "lengths_irony = [len(tweet.split()) for tweet in train_irony[\"tweets\"]]\n",
    "print(\"Average word for the atheism dataset:\", round(np.mean(lengths_atheism), 2))\n",
    "print(\"Average word for the irony dataset:\", round(np.mean(lengths_irony), 2),\"\\0\")\n",
    "print(\"On Average a tweet from the atheism dataset is using\",  round(np.mean(lengths_atheism)/np.mean(lengths_irony), 2), \n",
    "      \"times more words than from the the irony dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the evaluated dataset, people twitting about religion (or not being religous) wrote longer tweets than people expressing irony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top n words in each dataset\n",
    "for n in [5, 10, 20, 50, 100]:\n",
    "    top_atheism = list(atheism_vocabulary[\"word\"])[0:n]\n",
    "    top_irony = list(irony_vocabulary[\"word\"])[0:n]\n",
    "    top_frequency = len([word for word in top_atheism if word in top_irony])/len(top_atheism)\n",
    "    print(top_frequency*100, \"% of the top\", n, \"words in atheism dataset appear in the top\", n ,\"of the irony dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that most of the top words above top 5 are mostly the same, even when comparing completly different topics. This could mean that the context of the tweet is based on very few words, and the rest are just filler words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_irony = irony_vocabulary.iloc[:10,:2].reset_index(drop=True)\n",
    "top10_irony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_atheism = atheism_vocabulary.iloc[:10,:2].reset_index(drop=True)\n",
    "top10_atheism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As twitter usenames were all changed to 'user' we can see a big difference between user mentions in both datasets. For irony the word 'user' ranks number 1 with a frequency of 1668, and for atheism it ranks 7 with a frequency of 135.\n",
    "\n",
    "Even with the difference between corpus size, we can still see that use mentions are much higher in the irony dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_um = {'total_tweets': [len(train_irony), len(train_atheism)],\n",
    "        'total_words':[len(irony_tokens), len(atheism_tokens)],\n",
    "        'user_mentions':[top10_irony['frequency'][top10_irony['word'] == 'user'][0],\n",
    "         top10_atheism['frequency'][top10_atheism['word'] == 'user'][7]]\n",
    "        }\n",
    "\n",
    "user_mentions = pd.DataFrame(data_um, index =['irony_train',\n",
    "                                'atheism_train'\n",
    "                                ])\n",
    "  \n",
    "user_mentions['user_mentions_percentage'] = user_mentions['user_mentions']/user_mentions['total_words'] * 100\n",
    "\n",
    "user_mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences in balance/inbalance\n",
    "The irony dataset is mapped as follows:\n",
    "- 0\tnot ironic\n",
    "- 1\tironic\n",
    "\n",
    "The atheism dataset is mapped as follows:\n",
    "- 0\tnone\n",
    "- 1\tagainst\n",
    "- 2\tfavor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irony_train['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atheism_train['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the irony dataset has almost perfect balance, unlike the atheism dataset where we can see a big inbalance between classes.  \n",
    "\n",
    "And as there are many more tweets against atheism than neutral or in favour, this imbalance can become an issue because the predictive model would be biased towards predicting 'against' and to lesser degree to 'none' than 'favor'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Manual Annotation and Inter-Annotator Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smaller datasets, each consisting of 120 randomly selected tweets from each task’s training set, were provided to do manual annotation and Inter-Annotator Agreement thereof. The chosen task to do annotations was *Irony*. \n",
    "Before annotating an agreed upon scheme was discussed based on the research paper “[SemEval-2018 Task 3: Irony Detection in English Tweets](https://www.aclweb.org/anthology/S18-1005.pdf)” provided from the SemEval workshop, describing how each of the original datasets was created and annotated. Here it is stated that all annotations were done using the brat rapid annotation tool, a Web-based Tool for NLP-Assisted Text Annotation, specifically following the report “[Guidelines for Annotating Irony in Social Media Text](https://core.ac.uk/download/pdf/74589284.pdf)”. \n",
    "From this, we agreed on some definitions of irony:\n",
    "> Irony is the use of words to express something other than and especially the opposite of the literal meaning.\n",
    "\n",
    "Meaning we first and foremost were to annotate a tweet as being ironic, if the tweet was ironic by means of a literal meaning clash or not. <br>\n",
    "However, as the guide describes there are different forms of irony and an instance can therefore contain another from of irony, where there is no polarity clash, but the tweet still will be found ironic, e.g. situational irony.\n",
    "\n",
    "Therefore, when investigating the tweets, we were to think about whether the tweet was:\n",
    "- Ironic by means of a clash: the text expresses an evaluation whose literal polarity is opposite to the intended polarity. <code>1 - irony</code>\n",
    "- Other type of irony: there is no clash between the literal and the intended evaluation, but the text is still ironic. <code>1 - irony</code>\n",
    "- Not ironic: the text is not ironic. <code>0 - non_irony</code>\n",
    "\n",
    "Working independently each group member manually went through the sample and labelled them according to the agreed upon scheme. Next the Inter-annotator agreement (IAA) coefficients were computed using the [nltk.metrics.agreement module](https://www.nltk.org/api/nltk.metrics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in MAA.columns:\n",
    "    score = 0\n",
    "    for i in range(len(MAA[name])):\n",
    "        if list(MAA[name])[i] == list(MAA[\"truth\"])[i]:\n",
    "            score += 1\n",
    "    print(name, score/120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rasmus_labels = [(\"rasmus\", i, list(MAA[\"rasmus\"])[i]) for i in range(len(list(MAA[\"rasmus\"])))]\n",
    "sabrina_labels = [(\"sabrina\", i, list(MAA[\"sabrina\"])[i]) for i in range(len(list(MAA[\"sabrina\"])))]\n",
    "ruben_labels = [(\"ruben\", i, list(MAA[\"ruben\"])[i]) for i in range(len(list(MAA[\"ruben\"])))]\n",
    "ida_labels = [(\"ida\", i, list(MAA[\"ida\"])[i]) for i in range(len(list(MAA[\"ida\"])))]\n",
    "magnus_labels = [(\"magnus\", i, list(MAA[\"magnus\"])[i]) for i in range(len(list(MAA[\"magnus\"])))]\n",
    "truth_labels = [(\"truth\", i, list(MAA[\"truth\"])[i]) for i in range(len(list(MAA[\"truth\"])))]\n",
    "\n",
    "ls_w_truth = []\n",
    "ls_wo_truth = []\n",
    "\n",
    "for i in range(len(rasmus_labels)):\n",
    "    ls_w_truth.append(sabrina_labels[i])\n",
    "    ls_w_truth.append(rasmus_labels[i])\n",
    "    ls_w_truth.append(ruben_labels[i])\n",
    "    ls_w_truth.append(ida_labels[i])\n",
    "    ls_w_truth.append(magnus_labels[i])\n",
    "    ls_w_truth.append(truth_labels[i])\n",
    "    ls_wo_truth.append(sabrina_labels[i])\n",
    "    ls_wo_truth.append(rasmus_labels[i])\n",
    "    ls_wo_truth.append(ruben_labels[i])\n",
    "    ls_wo_truth.append(ida_labels[i])\n",
    "    ls_wo_truth.append(magnus_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix for the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = MAA[\"truth\"]\n",
    "\n",
    "for name in MAA.columns:\n",
    "    print(name, \"\\0\", confusion_matrix(y_true, MAA[name]), \"\\0\")\n",
    "\n",
    "print(\"[[\"\"non-irony labelled non-irony - non-irony labelled irony\"\"]\", \"\\0\", \"[ irony labelled non-irony - irony labelled irony]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First taking a look at the IAA coefficients when <u>not</u> including the truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AnnotationTask(data = ls_wo_truth)\n",
    "print(\"Average observed agreement:\", t.avg_Ao())\n",
    "print(\"S score:\", t.S())\n",
    "print(\"Scott's multi-pi score:\", t.pi())\n",
    "print(\"Cohen's kappa score:\", t.kappa())\n",
    "print(\"Fleiss' multi-kappa score:\", t.multi_kappa())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when including the true labels as an annotator, we get the following IAA coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_w_truth = AnnotationTask(data = ls_w_truth)\n",
    "print(\"Average observed agreement:\", t_w_truth.avg_Ao())\n",
    "print(\"S score:\", t_w_truth.S())\n",
    "print(\"Scott's multi-pi score:\", t_w_truth.pi())\n",
    "print(\"Cohen's kappa score:\", t_w_truth.kappa())\n",
    "print(\"Fleiss' multi-kappa score:\", t_w_truth.multi_kappa())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to remember that the original annotations are done by humans. They might have been through more training and been taking longer time to do the annotations, but they are not necessarily \"true annotations\". We therefore include them, as it is an indicator of the diffuculty of carrying out annotation tasks consistently. <br>\n",
    "<br>\n",
    "Finding the pair-wise kappa scores, mostly to make a figure for the report and seeing the distribution of agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [i for i in MAA.columns]\n",
    "list_of_labels = [sabrina_labels, rasmus_labels, ruben_labels, ida_labels, magnus_labels, truth_labels]\n",
    "\n",
    "total_sum = 0\n",
    "for i, name in enumerate(names):\n",
    "    print(\"\\0 name:\", name)\n",
    "    current = list_of_labels[i]\n",
    "    #print(current)\n",
    "    for idx in range(len(names)):\n",
    "        if idx != i:\n",
    "            ls = []\n",
    "            pair_with_name = names[idx]\n",
    "            pair_with = list_of_labels[idx]\n",
    "            for annotation in range(len(current)):\n",
    "                ls.append(current[annotation])\n",
    "                ls.append(pair_with[annotation])\n",
    "            #print(ls)\n",
    "            t = AnnotationTask(data = ls)\n",
    "            print(names[idx], t.kappa())\n",
    "            total_sum += t.kappa()\n",
    "\n",
    "print(\"\\0Average kappa:\", total_sum/30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Landis and Koch, Biometrics 1977)*\n",
    "<div>\n",
    "<img src=\"../figures/LandisandKoch.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agreement Without Chance Correction**<br>\n",
    "The group got an **average observed agreement** of 0.60, which according to the rule of thumb given by Landis and Koch-scale is on the edge between being a moderate and substantial level of agreement.\n",
    "The average observed agreement between the group members can however be due to chance, as the Irony dataset only has two possible labels, causing the probability of getting a label right by chance to be high. <br><br>\n",
    "$$A_O = \\frac{\\text{no. matches}}{\\text{no. total items}}$$\n",
    "\n",
    "In the NLTK library we used the function <code>avg_o()</code> which computed the *“average observed agreement across all coders and items.”* <br>\n",
    "General problems with the observed agreement is the bias in favor of dimensions with a small number of categories. Therefore also the problem of getting figures that are comparable across studies of different dimensions with different number of categories, as it must be adjusted for chance agreement to do so. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chance-corrected Agreement** <br>\n",
    "The three best-known coefficients, $S$ <I>(Bennett, Alpert, and Goldstein 1954)</I>, $\\pi$ <I>(Scott 1955)</I>, and $\\kappa$ <I>(Cohen 1960)</I> is in it’s basic form used when measuring agreement between two coders. Yet the NLTK library made it possible for us to use with more than two coders. \n",
    "\n",
    "The above mentioned coefficients use the following formula:\n",
    "$$ S, \\pi, \\kappa = \\frac{\\text{observed agreement} - \\text{expected agreement}}{1 - \\text{expected agreement}}$$\n",
    "\n",
    "The difference between $S$, $\\pi$ and $\\kappa$ lies in the assumptions to the calculation of the chance of coder $c_i$ assigning an arbitrary item to category $k$, as follows:\n",
    "* Coefficient of $S$ is based on the assumption that if coders were operating by chance alone, we would get a uniform distribution.\n",
    "* Coefficient of $\\pi$ is based on the assumption that if coders were operating by chance one, we would get the same distribution of each coder. \n",
    "* Coefficient of $\\kappa$ is based on the assumption that if coders were operating by chance alone, we would get a separate distribution for each coder.\n",
    "\n",
    "From: [Inter-Coder Agreement for Computational Linguistics](https://www.aclweb.org/anthology/J08-4004.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the NLTK librabry these functions were used to compute the following with the corresponding scores as a result:\n",
    "* <code>S()</code>: 0.2656 (Bennett, Albert and Goldstein)\n",
    "* <code>pi()</code>: 0.2577 (Scott; here, multi-pi. Equivalent to K from Siegel and Castellan)\n",
    "* <code>kappa()</code>: 0.2923 (Cohen. Averages naively over kappas for each coder pair)\n",
    "* <code>multi-kappa()</code>: 0.2770 (Davies and Fleiss. Averages over observed and expected agreements for each coder pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inter-annotator agreement problematics**\n",
    "\n",
    "Common reasons for low intra-annotor agreement scores is:\n",
    "* ill-defined guidelines\n",
    "* use of specific priming effects\n",
    "* insuficient information to make decisions\n",
    "* different annotator background\n",
    "* different understanding of the task \n",
    "\n",
    "In general Inter-Annotator Agreement gives an indication of how well-defined and reproducible the task is. In this case it shows that the task of detecting irony in writing and especially a somewhat short text as a tweet is difficult even for the human reader. When the group discussed the annotation of the dataset afterwards it was agreed upon that possible context was lacking.\n",
    "As mentioned earlier all instances of present irony was not necessarily being in the means of a clash, but also other type of irony, making the guidelines possibly too loose and cause of diverging interpretations of when something is ironic or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Automatic Prediction\n",
    "---\n",
    "This section is split into two parts, Binary Classification on Irony Dataset, and Multiclass-classification on the Stance Dataset. The muliticlass-classification is further split into two parts, doing classification on a sub-stance (Atheism) and classifying whether a tweet is a specific stance with a model trained on all of the different stances at the same time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification on Irony Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the <code>tokenize_ideal</code> function from earlier to tokenize the data and converting it to be an useable datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(file_name):\n",
    "    with open(file_name, 'r', encoding=\"utf8\") as f:\n",
    "        data = f.read().split(\"\\n\")\n",
    "        corpus = []\n",
    "        for line in data:\n",
    "            ls = tokenize_ideal(line)\n",
    "            corpus.append(ls[0])\n",
    "        \n",
    "        if corpus[-1] == \"\":\n",
    "            return corpus[:-1]\n",
    "        \n",
    "        return corpus\n",
    "\n",
    "train_corpus = tokenizer(\"./datasets/irony/train_text.txt\")\n",
    "test_corpus = tokenizer(\"./datasets/irony/test_text.txt\")\n",
    "validation_corpus = tokenizer(\"./datasets/irony/val_text.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the train labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_data(location):\n",
    "    y_train = []\n",
    "    for i in open(location):\n",
    "        y_train.append(int(i.replace(\"\\n\",\"\")))\n",
    "    \n",
    "    return y_train\n",
    "\n",
    "y_train = y_data(\"./datasets/irony/train_labels.txt\")\n",
    "y_test = y_data(\"./datasets/irony/test_labels.txt\")\n",
    "y_val = y_data(\"./datasets/irony/val_labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced data, thus we try and optimize the accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the pipeline, to make it easier to preprocess, train/fit and predict on data. Having it be a function to reduce duplicate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(model):\n",
    "    return Pipeline([('vect', CountVectorizer(preprocessor = lambda x: x, tokenizer = lambda x: x,token_pattern=None)),\n",
    "                                             ('tfidf', TfidfTransformer()), \n",
    "                                             ('clf', model)\n",
    "                                             ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our baseline model on our tokenized training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = model_pipeline(SGDClassifier(loss='log_loss', random_state=42)).fit(train_corpus, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = LR.predict(validation_corpus)\n",
    "accuracy_score(predicted, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy_score of 0.556 is not the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification models different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training different classifying models without any special tuning to see which is the best in its simplest form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = model_pipeline(KNeighborsClassifier()).fit(train_corpus, y_train)\n",
    "DTC = model_pipeline(DecisionTreeClassifier(random_state=42)).fit(train_corpus, y_train)\n",
    "RFC = model_pipeline(RandomForestClassifier(random_state=42)).fit(train_corpus, y_train)\n",
    "SV  = model_pipeline(SVC(random_state=42)).fit(train_corpus, y_train)\n",
    "NB  = model_pipeline(MultinomialNB()).fit(train_corpus,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsc = [LR, KNN, DTC, RFC, SV, NB]\n",
    "d = [\"LogisticRegression\",\n",
    "     \"KNearestNeighbor\",\n",
    "     \"DecisionTreeClassifier\",\n",
    "     \"RandomForestClassifier\",\n",
    "     \"SupportVectorClassifier\",\n",
    "     \"NaiveBayes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the different metrics that are useable and give insight into how good the specific model is. Here we mainly look at accuracy_score and f1 score since recall and precision can be \"gamed\" but they can not both be gamed at the same time, thus using the f1 score gives credible insight into both recall and precision and thus an overall score for the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is the proportion of elements classified correctly:\n",
    "\n",
    "$$Accuracy = \\frac{\\text{sum of diagonal}}{\\text{total sum}}$$\n",
    "\n",
    "Accuracy can be very misleading in unbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision: Out of the examples we **predicted to be** in a certain class, how many of them are correct?\n",
    "<br><br>\n",
    "$$\n",
    "Precision=\\frac{\\text{single diagonal element}}{\\text{sum of a single column} }\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall: Out of the examples **that actually belong** to a certain class, how many of them did we find?\n",
    "<br><br>\n",
    "$$\n",
    "Recall = \\frac{\\text{single diagonal element}}{\\text{sum of a single row} }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-score: Harmonic mean of Precision and Recall\n",
    "\n",
    "$$\n",
    "F_{score} = 2 \\cdot \\frac{P\\cdot R}{P+R}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model in enumerate(modelsc):\n",
    "    name = d[idx]\n",
    "    predict = model.predict(validation_corpus)\n",
    "    print(str(name) + \": \")\n",
    "    print(\"     - accuracy score:\\t   {}\"  .format(accuracy_score( y_val, predict)))\n",
    "    print(\"     - recall score: \\t   {}\"   .format(recall_score(   y_val, predict)))\n",
    "    print(\"     - precision score:\\t   {}\" .format(precision_score(y_val, predict)))\n",
    "    print(\"     - f1 score:\\t   {}\"        .format(f1_score(       y_val, predict)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now doing cross-validation on the validation dataset and taking the mean of the cross-validation-scores, we can find the overall best models for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(columns=[\"MODELS\",\"R2CV\"])\n",
    "for i, model in enumerate(modelsc):\n",
    "    name = d[i]\n",
    "    R2CV = cross_val_score(model,validation_corpus,y_val,cv=2).mean()\n",
    "    result = pd.DataFrame([[name,R2CV*100]],columns=[\"MODELS\",\"R2CV\"])\n",
    "    r = r.append(result)\n",
    "\n",
    "figure = plt.figure(figsize=(20,8))\n",
    "sns.barplot(x=\"R2CV\",y=\"MODELS\",data=r,color=\"k\")\n",
    "plt.xlabel(\"Cross Val Score\")\n",
    "plt.ylabel(\"MODELS\")\n",
    "plt.xlim(0,100)\n",
    "plt.title(\"MODEL ACCURACY COMPARISON\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that none of our models perform particularly well, though we still have some models performing better than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the best models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to find the best possible combination of parameters to achieve the highest F-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "accuracyscore = {}\n",
    "recallscore = {}\n",
    "precisionscore = {}\n",
    "fscore = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing a simple RFC model (no parameters), that can be tweaked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_noob = model_pipeline(RandomForestClassifier(random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing all possible combinations of chosen parameters and finding the F-score (very time consuming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"clf__n_estimators\": [100, 500, 1000, 2000],\n",
    "             \"clf__max_depth\": [3, 6, 12, 24, 48],\n",
    "             \"clf__criterion\": [\"gini\", \"entropy\"],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(RFC_noob, parameters, cv=2, n_jobs=-1).fit(train_corpus, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"RFC\"] = gs_clf.best_score_\n",
    "scores[\"RFC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_tuned = Pipeline([('vect', CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, token_pattern=None)\n",
    "                     ), ('tfidf', TfidfTransformer()\n",
    "                     ), ('clf', RandomForestClassifier(random_state=42, \n",
    "                                                       criterion = 'gini', \n",
    "                                                       max_depth= 12, \n",
    "                                                       n_estimators= 100))]).fit(train_corpus, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = RFC_tuned.predict(validation_corpus)\n",
    "accuracyscore[\"RandomForestClassifier\"] = accuracy_score(y_val ,predict) \n",
    "recallscore[\"RandomForestCl*assifier\"] = recall_score(y_val, predict)\n",
    "precisionscore[\"RandomForestClassifier\"] = precision_score(y_val, predict)\n",
    "fscore[\"RandomForestClassifier\"] = f1_score(y_val, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing a simple Support Vector model (no parameters), that can be tweaked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SV_noob = model_pipeline(SVC(random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing all possible combinations of chosen parameters and finding the F-score (very time consuming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"clf__C\": [1,3,9,27],\n",
    "              \"clf__kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "              \"clf__degree\": [1,3,9,27],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(SV_noob, parameters, cv=2, n_jobs=-1).fit(train_corpus, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"SV\"] = gs_clf.best_score_\n",
    "scores[\"SV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SV_tuned = Pipeline([('vect', CountVectorizer(tokenizer=lambda x: x, preprocessor= lambda x: x, token_pattern=None )\n",
    "                     ), ('tfidf', TfidfTransformer()\n",
    "                     ), ('clf', SVC(random_state=42, \n",
    "                                    C = 1, \n",
    "                                    degree= 1,\n",
    "                                    kernel=\"rbf\"))]).fit(train_corpus, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = SV_tuned.predict(validation_corpus)\n",
    "accuracyscore[\"SupportVector\"] = accuracy_score(y_val ,predict) \n",
    "recallscore[\"SupportVector\"] = recall_score(y_val, predict)\n",
    "precisionscore[\"SupportVector\"] = precision_score(y_val, predict)\n",
    "fscore[\"SupportVector\"] = f1_score(y_val, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_noob = model_pipeline(SGDClassifier(loss='log_loss', random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"clf__alpha\": [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03],\n",
    "             \"clf__penalty\": [\"l2\", \"l1\", \"elasticnet\"],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(LR_noob, parameters, cv=2, n_jobs=-1).fit(train_corpus, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"LR\"] = gs_clf.best_score_\n",
    "scores[\"LR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_tuned = Pipeline([('vect', CountVectorizer(tokenizer=lambda x: x, preprocessor= lambda x: x, token_pattern=None )\n",
    "                     ), ('tfidf', TfidfTransformer()\n",
    "                     ), ('clf', SGDClassifier(loss=\"log\", \n",
    "                                    random_state=42,\n",
    "                                    alpha=0.0003, \n",
    "                                    penalty= \"l2\"))]).fit(train_corpus, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = LR_tuned.predict(validation_corpus)\n",
    "accuracyscore[\"LogisticRegression\"] = accuracy_score(y_val ,predict) \n",
    "recallscore[\"LogisticRegression\"] = recall_score(y_val, predict)\n",
    "precisionscore[\"LogisticRegression\"] = precision_score(y_val, predict)\n",
    "fscore[\"LogisticRegression\"]= f1_score(y_val, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final model on the basis of the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(15,8))\n",
    "ax[0,0].set_title(\"Accuracy score\")\n",
    "ax[0,1].set_title(\"F-score\")\n",
    "ax[1,0].set_title(\"Precision\")\n",
    "ax[1,1].set_title(\"Recall\")\n",
    "\n",
    "for i in accuracyscore.items():\n",
    "    ax[0,0].bar(i[0], i[1])\n",
    "    ax[0,0].set_ylim([0.4,0.6])\n",
    "\n",
    "for i in fscore.items():\n",
    "    ax[0,1].bar(i[0], i[1])\n",
    "    ax[0,1].set_ylim([0.4,0.7])\n",
    "    \n",
    "for i in precisionscore.items():\n",
    "    ax[1,0].bar(i[0], i[1])\n",
    "    ax[1,0].set_ylim([0.4,0.7])\n",
    "    \n",
    "for i in recallscore.items():\n",
    "    ax[1,1].bar(i[0], i[1])\n",
    "    ax[1,1].set_ylim([0.4,0.8])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the models that we have tuned to have the highest accuracy score are all equally bad at classifying whether a given sentence is ironic.\n",
    "\n",
    "It is logical that our model is really bad at classifying text-based irony, since the group members are also very poor at classifying irony. Thus we can not expect the model to outperform us massively, since it lacks world knowledge and general experience and the emotional aspect of understanding the users intentions. Perhabs combining an intention model and an irony model could increase the performance of the irony model.\n",
    "\n",
    "Final model is LogisticRegression, since it has an higher accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final model on test data metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RFC_tuned\n",
    "predict = model.predict(test_corpus)\n",
    "print(\"Logistic Regression\" + \": \")\n",
    "print(\"     - accuracy score:\\t   {}\"  .format(accuracy_score( y_test, predict)))\n",
    "print(\"     - recall score: \\t   {}\"   .format(recall_score(   y_test, predict)))\n",
    "print(\"     - precision score:\\t   {}\" .format(precision_score(y_test, predict)))\n",
    "print(\"     - f1 score:\\t   {}\"        .format(f1_score(       y_test, predict)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Classification on Stance Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Multi-classification, mapping all the different stances to a 3x5 map taking values from 0-14 and classifying on the basis of these 15 different labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the neccesery dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hillary_train  = pd.concat([train_hillary,  train_hillary_labels ], axis=1)\n",
    "abortion_train = pd.concat([train_abortion, train_abortion_labels], axis=1)\n",
    "atheism_train  = pd.concat([train_atheism,  train_atheism_labels ], axis=1)\n",
    "climate_train  = pd.concat([train_climate,  train_climate_labels ], axis=1)\n",
    "feminist_train = pd.concat([train_feminist, train_feminist_labels], axis=1)\n",
    "\n",
    "abortion_train[\"labels\"] += 3\n",
    "atheism_train[\"labels\"]  += 6\n",
    "climate_train[\"labels\"]  += 9\n",
    "feminist_train[\"labels\"] += 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hillary_val  = pd.concat([validation_hillary,  validation_hillary_labels ], axis=1)\n",
    "abortion_val = pd.concat([validation_abortion, validation_abortion_labels], axis=1)\n",
    "atheism_val  = pd.concat([validation_atheism,  validation_atheism_labels ], axis=1)\n",
    "climate_val  = pd.concat([validation_climate,  validation_climate_labels ], axis=1)\n",
    "feminist_val = pd.concat([validation_feminist, validation_feminist_labels], axis=1)\n",
    "\n",
    "abortion_val[\"labels\"] += 3\n",
    "atheism_val[\"labels\"]  += 6\n",
    "climate_val[\"labels\"]  += 9\n",
    "feminist_val[\"labels\"] += 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hillary_test  = pd.concat([test_hillary,  test_hillary_labels ], axis=1)\n",
    "abortion_test = pd.concat([test_abortion, test_abortion_labels], axis=1)\n",
    "atheism_test  = pd.concat([test_atheism,  test_atheism_labels ], axis=1)\n",
    "climate_test  = pd.concat([test_climate,  test_climate_labels ], axis=1)\n",
    "feminist_test = pd.concat([test_feminist, test_feminist_labels], axis=1)\n",
    "\n",
    "abortion_test[\"labels\"] += 3\n",
    "atheism_test[\"labels\"]  += 6\n",
    "climate_test[\"labels\"]  += 9\n",
    "feminist_test[\"labels\"] += 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping = {}\n",
    "#mapping[0]  = \"none hillary\"\n",
    "#mapping[1]  = \"against hillary\"\n",
    "#mapping[2]  = \"favor hillary\"\n",
    "#mapping[3]  = \"none abortion\"\n",
    "#mapping[4]  = \"against abortion\"\n",
    "#mapping[5]  = \"favor abortion\"\n",
    "#mapping[6]  = \"none atheism\"\n",
    "#mapping[7]  = \"against atheism\"\n",
    "#mapping[8]  = \"favor atheism\"\n",
    "#mapping[9]  = \"none climate\"\n",
    "#mapping[10] = \"against climate\"\n",
    "#mapping[11] = \"favor climate\"\n",
    "#mapping[12] = \"none feminist\"\n",
    "#mapping[13] = \"against feminist\"\n",
    "#mapping[14] = \"favor feminist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train = pd.read_csv(PATHS[\"combined_stance\"].format(\"combined_train.csv\"))\n",
    "combined_train = combined_train.drop('Unnamed: 0', 1)\n",
    "\n",
    "combined_val = pd.read_csv(PATHS[\"combined_stance\"].format(\"combined_val.csv\"))\n",
    "combined_val = combined_val.drop('Unnamed: 0', 1)\n",
    "\n",
    "combined_test = pd.read_csv(PATHS[\"combined_stance\"].format(\"combined_test.csv\"))\n",
    "combined_test = combined_test.drop('Unnamed: 0', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing our training and validation tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_train = combined_train\n",
    "for idx, i in enumerate(copy_train[\"tweets\"]):\n",
    "    x = tokenize_ideal(i)\n",
    "    combined_train.loc[idx, [\"tweets\"]] = x[0]\n",
    "\n",
    "copy_val = combined_val\n",
    "for idx, i in enumerate(copy_val[\"tweets\"]):\n",
    "    x = tokenize_ideal(i)\n",
    "    combined_val.loc[idx, [\"tweets\"]] = x[0]\n",
    "\n",
    "copy_test = combined_test\n",
    "for idx, i in enumerate(copy_test[\"tweets\"]):\n",
    "    x = tokenize_ideal(i)\n",
    "    combined_test.loc[idx, [\"tweets\"]] = x[0]\n",
    "    \n",
    "\n",
    "combined_val   = combined_val[combined_val['labels'].notna()]\n",
    "combined_train = combined_train[combined_train[\"labels\"].notna()]\n",
    "combined_test  = combined_test[combined_test[\"labels\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the data is not balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_multi = model_pipeline(SGDClassifier(loss='log_loss', random_state=42)).fit(combined_train[\"tweets\"], combined_train[\"labels\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = LR_multi.predict(combined_val[\"tweets\"])\n",
    "accuracy_score(predicted, combined_val[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification models different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training different classifying models without any special tuning to see which is the best in its simplest form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_multi = model_pipeline(KNeighborsClassifier()).fit(combined_train[\"tweets\"], combined_train[\"labels\"])\n",
    "DTC_multi = model_pipeline(DecisionTreeClassifier(random_state=42)).fit(combined_train[\"tweets\"], combined_train[\"labels\"])\n",
    "RFC_multi = model_pipeline(RandomForestClassifier(random_state=42)).fit(combined_train[\"tweets\"], combined_train[\"labels\"])\n",
    "SV_multi  = model_pipeline(SVC(random_state=42)).fit(combined_train[\"tweets\"], combined_train[\"labels\"])\n",
    "NB_multi  = model_pipeline(MultinomialNB()).fit(combined_train[\"tweets\"],combined_train[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsc = [LR_multi, KNN_multi, DTC_multi, RFC_multi, SV_multi, NB_multi]\n",
    "d = [\"LogisticRegression\",\n",
    "     \"KNearestNeighbor\",\n",
    "     \"DecisionTreeClassifier\",\n",
    "     \"RandomForestClassifier\",\n",
    "     \"SupportVectorClassifier\",\n",
    "     \"NaiveBayes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the precision score will be flawed since our models does not predict all labels in the dataset, i.e the NaiveBayes only predicts 5 out of the 15 different labels in our data, which makes calculating the precision score for these non-predicted labels not possible, since they have not been predicted. This is also the reason that warnings is being returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = list(range(0,15))\n",
    "for idx, model in enumerate(modelsc):\n",
    "    name = d[idx]\n",
    "    predict = model.predict(combined_val[\"tweets\"])\n",
    "    uniq, counts = np.unique(predict, return_counts=True)\n",
    "    print(str(name) + \": \")\n",
    "    temp = [item for item in ls if item not in uniq]\n",
    "    print(\"     - classes not predicted by model: \\t {}\".format(temp))\n",
    "    print(\"     - accuracy score:\\t   {}\"  .format(accuracy_score( combined_val[\"labels\"],predict)))\n",
    "    print(\"     - recall score: \\t   {}\"   .format(recall_score(   combined_val[\"labels\"], predict, average='macro')))\n",
    "    print(\"     - precision score:\\t   {}\" .format(precision_score(combined_val[\"labels\"], predict, average='macro')))\n",
    "    print(\"     - f1 score:\\t   {}\"        .format(f1_score(       combined_val[\"labels\"], predict, average='macro')))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our validation data, we have a label that is only occuring twice in our data, which means that we can not perform any proper cross-validation scores on it, as we simply do not have enough data to support any conclusions. This is also the reason that a lot of our classifiers is ignoring these classes with few members, because we train on data that does not have many members of that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_val[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cross validation is thus flawed since a lot of our models is ignoring a lot of the classes in our data, such as the NaiveBayes is ignoring more than half of the classes in the data. This makes it a very bad model for classifying stance, but in the cross validation it, at first glance, looks to have a very high score, which is very misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(columns=[\"MODELS\",\"R2CV\"])\n",
    "for i, model in enumerate(modelsc):\n",
    "    name = d[i]\n",
    "    R2CV = cross_val_score(model, combined_val[\"tweets\"], combined_val[\"labels\"], cv=2).mean()\n",
    "    result = pd.DataFrame([[name,R2CV*100]], columns=[\"MODELS\",\"R2CV\"])\n",
    "    r = r.append(result)\n",
    "\n",
    "figure = plt.figure(figsize=(20, 8))\n",
    "sns.barplot(x=\"R2CV\", y=\"MODELS\", data=r, color=\"k\")\n",
    "plt.xlabel(\"Cross Val Score\")\n",
    "plt.ylabel(\"MODELS\")\n",
    "plt.xlim(0, 100)\n",
    "plt.title(\"MODEL ACCURACY COMPARISON\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the best models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then decide to try and optimize the models that actually predict most of the classes in our dataset. \n",
    "\n",
    "Now we try to find the best possible combination of parameters to achieve the highest F-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "accuracyscore = {}\n",
    "recallscore = {}\n",
    "precisionscore = {}\n",
    "fscore = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_noob_multi = model_pipeline(SGDClassifier(loss='log_loss', random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"clf__alpha\": [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03],\n",
    "              \"clf__penalty\": [\"l2\", \"l1\", \"elasticnet\"],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(LR_noob_multi, parameters, cv=2, n_jobs=-1, scoring=\"recall_macro\").fit(combined_train[\"tweets\"], combined_train[\"labels\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"LR\"] = gs_clf.best_score_\n",
    "scores[\"LR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_tuned_multi = Pipeline([('vect', CountVectorizer(tokenizer=lambda x: x, preprocessor= lambda x: x, token_pattern=None )\n",
    "                     ), ('tfidf', TfidfTransformer()\n",
    "                     ), ('clf', SGDClassifier(loss=\"log\",\n",
    "                                    random_state=42,\n",
    "                                    alpha=0.0001, \n",
    "                                    penalty= \"l1\"))]).fit(combined_train[\"tweets\"], combined_train[\"labels\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = LR_tuned_multi.predict(combined_val[\"tweets\"])\n",
    "ls = list(range(0,15))\n",
    "uniq, counts = np.unique(predict, return_counts=True)\n",
    "temp = [item for item in ls if item not in uniq]\n",
    "print(\"classes not predicted by model: \\t {}\".format(temp))\n",
    "accuracyscore[\"LogisticRegression\"]  = accuracy_score( combined_val[\"labels\"] , predict) \n",
    "recallscore[\"LogisticRegression\"]    = recall_score(   combined_val[\"labels\"],  predict, average=\"macro\")\n",
    "precisionscore[\"LogisticRegression\"] = precision_score(combined_val[\"labels\"],  predict, average=\"macro\")\n",
    "fscore[\"LogisticRegression\"]         = f1_score(       combined_val[\"labels\"],  predict, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_noob_multi = model_pipeline(RandomForestClassifier(random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"clf__n_estimators\": [100, 500, 1000, 2000],\n",
    "             \"clf__max_depth\": [3, 6, 12, 24, 48],\n",
    "             \"clf__criterion\": [\"gini\", \"entropy\"],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(RFC_noob_multi, parameters, cv=2, n_jobs=-1, scoring=\"recall_macro\").fit(combined_train[\"tweets\"], combined_train[\"labels\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"RFC\"] = gs_clf.best_score_\n",
    "scores[\"RFC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_tuned_multi = Pipeline([('vect', CountVectorizer(tokenizer=lambda x: x, preprocessor= lambda x: x, token_pattern=None)\n",
    "                         ), ('tfidf', TfidfTransformer()\n",
    "                         ), ('clf', RandomForestClassifier(random_state=42, \n",
    "                                                       criterion = 'gini', \n",
    "                                                       max_depth= 48, \n",
    "                                                       n_estimators= 500))]).fit(combined_train[\"tweets\"], combined_train[\"labels\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = RFC_tuned_multi.predict(combined_val[\"tweets\"])\n",
    "ls = list(range(0,15))\n",
    "uniq, counts = np.unique(predict, return_counts=True)\n",
    "temp = [item for item in ls if item not in uniq]\n",
    "print(\"classes not predicted by model: {}\".format(temp))\n",
    "accuracyscore[\"RandomForestClassifier\"] = accuracy_score(combined_val[\"labels\"], predict) \n",
    "recallscore[\"RandomForestClassifier\"] = recall_score(combined_val[\"labels\"], predict, average=\"macro\")\n",
    "precisionscore[\"RandomForestClassifier\"] = precision_score(combined_val[\"labels\"], predict, average=\"macro\")\n",
    "fscore[\"RandomForestClassifier\"] = f1_score(combined_val[\"labels\"], predict, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNearestNeighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_noob_multi = model_pipeline(KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"clf__n_neighbors\": [1,3,9,27,60,120],\n",
    "             \"clf__weights\": [\"uniform\", \"distance\"],\n",
    "             \"clf__algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(KNN_noob_multi, parameters, cv=2, n_jobs=-1, scoring=\"recall_macro\").fit(combined_train[\"tweets\"], combined_train[\"labels\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"KNN\"] = gs_clf.best_score_\n",
    "scores[\"KNN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_tuned_multi = Pipeline([('vect', CountVectorizer(tokenizer=lambda x: x, preprocessor= lambda x: x, token_pattern=None)\n",
    "                         ), ('tfidf', TfidfTransformer()\n",
    "                         ), ('clf', KNeighborsClassifier(algorithm= 'auto', \n",
    "                                                         n_neighbors=9,\n",
    "                                                         weights=\"distance\"))]).fit(combined_train[\"tweets\"],combined_train[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = KNN_tuned_multi.predict(combined_val[\"tweets\"])\n",
    "ls = list(range(0,15))\n",
    "uniq, counts = np.unique(predict, return_counts=True)\n",
    "temp = [item for item in ls if item not in uniq]\n",
    "print(\"classes not predicted by model: {}\".format(temp))\n",
    "accuracyscore[\"KNN\"]  = accuracy_score( combined_val[\"labels\"] , predict) \n",
    "recallscore[\"KNN\"]    = recall_score(   combined_val[\"labels\"],  predict, average=\"macro\")\n",
    "precisionscore[\"KNN\"] = precision_score(combined_val[\"labels\"],  predict, average=\"macro\")\n",
    "fscore[\"KNN\"]         = f1_score(       combined_val[\"labels\"],  predict, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(15,8))\n",
    "ax[0,0].set_title(\"Accuracy score\")\n",
    "ax[0,1].set_title(\"F-score\")\n",
    "ax[1,0].set_title(\"Precision\")\n",
    "ax[1,1].set_title(\"Recall\")\n",
    "\n",
    "for i in accuracyscore.items():\n",
    "    ax[0,0].bar(i[0], i[1])\n",
    "    ax[0,0].set_ylim([0,0.3])\n",
    "\n",
    "for i in fscore.items():\n",
    "    ax[0,1].bar(i[0], i[1])\n",
    "    ax[0,1].set_ylim([0,0.3])\n",
    "    \n",
    "for i in precisionscore.items():\n",
    "    ax[1,0].bar(i[0], i[1])\n",
    "    ax[1,0].set_ylim([0,0.3])\n",
    "    \n",
    "for i in recallscore.items():\n",
    "    ax[1,1].bar(i[0], i[1])\n",
    "    ax[1,1].set_ylim([0,0.3])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the KNN is the best model for our multiclass 3x5 stance mapping, since it only excludes one classification out of the total 15. The accuracy of the KNN is surprisingly high, given the difficulty of the task with this many different classes. Also if one were to randomnly guess (uniformly) what the class was, the probability of getting one correct would be $\\frac{1}{15}$ which our models are a lot better than. Meaning our models, even though  very bad, are actually better than random.\n",
    "\n",
    "Final model: KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final model on test data metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNN_tuned_multi\n",
    "predict = model.predict(combined_test[\"tweets\"])\n",
    "print(\"KNN\" + \": \")\n",
    "uniq, counts = np.unique(predict, return_counts=True)\n",
    "temp = [item for item in ls if item not in uniq]\n",
    "print(\"     - classes not predicted by model: \\t {}\".format(temp))\n",
    "print(\"     - accuracy score:\\t   {}\"  .format(accuracy_score( combined_test[\"labels\"], predict)))\n",
    "print(\"     - recall score: \\t   {}\"   .format(recall_score(   combined_test[\"labels\"], predict,  average=\"macro\")))\n",
    "print(\"     - precision score:\\t   {}\" .format(precision_score(combined_test[\"labels\"], predict,  average=\"macro\")))\n",
    "print(\"     - f1 score:\\t   {}\"        .format(f1_score(       combined_test[\"labels\"], predict,  average=\"macro\")))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking what subjects test data it performs well on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By only predicting one of the validation datasets using the multiclass 3x5 mapping, we can test what classes it performs well and poorly on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can not look at the precision and the recall scores, since it will take into account all of the different classes that we are not trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [hillary_val, abortion_val, atheism_val, climate_val, feminist_val]\n",
    "d = [\"hillary\", \"abortion\", \"atheism\", \"climate\", \"feminist\"]\n",
    "for idx, data in enumerate(ls):\n",
    "    name = d[idx]\n",
    "    predicted = model.predict(data[\"tweets\"])\n",
    "    print(str(name) + \": \")\n",
    "    print(\"     - accuracy score:\\t   {}\"  .format(accuracy_score(data[\"labels\"],predicted)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the model varies a lot on the different subject stances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-classification on a single stance subject (Atheism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atheism_train = pd.concat([train_atheism, train_atheism_labels], axis=1)\n",
    "atheism_val = pd.concat([validation_atheism, validation_atheism_labels], axis=1)\n",
    "atheism_test = pd.concat([test_atheism, test_atheism_labels], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing our training and validation tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_train = atheism_train\n",
    "for idx, i in enumerate(copy_train[\"tweets\"]):\n",
    "    x = tokenize_ideal(i)\n",
    "    atheism_train.loc[idx, [\"tweets\"]] = x[0]\n",
    "\n",
    "copy_val = atheism_val\n",
    "for idx, i in enumerate(copy_val[\"tweets\"]):\n",
    "    x = tokenize_ideal(i)\n",
    "    atheism_val.loc[idx, [\"tweets\"]] = x[0]\n",
    "\n",
    "copy_test = atheism_test\n",
    "for idx, i in enumerate(copy_test[\"tweets\"]):\n",
    "    x = tokenize_ideal(i)\n",
    "    atheism_test.loc[idx, [\"tweets\"]] = x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atheism_train[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is imbalanced, thus we use macro averaged recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_single = model_pipeline(SGDClassifier(loss='log_loss', random_state=42)).fit(atheism_train[\"tweets\"], atheism_train[\"labels\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = LR_single.predict(atheism_val[\"tweets\"])\n",
    "accuracy_score(predicted, atheism_val[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification models different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training different classifying models without any special tuning to see which is the best in its simplest form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_single = model_pipeline(KNeighborsClassifier()).fit(atheism_train[\"tweets\"], atheism_train[\"labels\"])\n",
    "DTC_single = model_pipeline(DecisionTreeClassifier(random_state=42)).fit(atheism_train[\"tweets\"], atheism_train[\"labels\"])\n",
    "RFC_single = model_pipeline(RandomForestClassifier(random_state=42)).fit(atheism_train[\"tweets\"], atheism_train[\"labels\"])\n",
    "SV_single  = model_pipeline(SVC(random_state=42)).fit(atheism_train[\"tweets\"], atheism_train[\"labels\"])\n",
    "NB_single  = model_pipeline(MultinomialNB()).fit(atheism_train[\"tweets\"],atheism_train[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsc = [LR_single, KNN_single, DTC_single, RFC_single, SV_single, NB_single]\n",
    "d = [\"LogisticRegression\",\n",
    "     \"KNearestNeighbor\",\n",
    "     \"DecisionTreeClassifier\",\n",
    "     \"RandomForestClassifier\",\n",
    "     \"SupportVectorClassifier\",\n",
    "     \"NaiveBayes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = list(range(0,3))\n",
    "for idx, model in enumerate(modelsc):\n",
    "    name = d[idx]\n",
    "    predict = model.predict(atheism_val[\"tweets\"])\n",
    "    print(str(name) + \": \")\n",
    "    uniq, counts = np.unique(predict, return_counts=True)\n",
    "    temp = [item for item in ls if item not in uniq]\n",
    "    print(\"     - classes not predicted by model: \\t {}\".format(temp))\n",
    "    print(\"     - accuracy score:\\t   {}\"  .format(accuracy_score(atheism_val[\"labels\"],predict)))\n",
    "    print(\"     - recall score: \\t   {}\"   .format(recall_score(atheism_val[\"labels\"], predict, average='macro')))\n",
    "    print(\"     - precision score:\\t   {}\" .format(precision_score(atheism_val[\"labels\"], predict, average='macro')))\n",
    "    print(\"     - f1 score:\\t   {}\"        .format(f1_score(atheism_val[\"labels\"], predict, average='macro')))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(columns=[\"MODELS\",\"R2CV\"])\n",
    "for i, model in enumerate(modelsc):\n",
    "    name = d[i]\n",
    "    R2CV = cross_val_score(model, atheism_val[\"tweets\"], atheism_val[\"labels\"], cv=2).mean()\n",
    "    result = pd.DataFrame([[name,R2CV*100]], columns=[\"MODELS\",\"R2CV\"])\n",
    "    r = r.append(result)\n",
    "\n",
    "figure = plt.figure(figsize=(20, 8))\n",
    "sns.barplot(x=\"R2CV\", y=\"MODELS\", data=r, color=\"k\")\n",
    "plt.xlabel(\"Cross Val Score\")\n",
    "plt.ylabel(\"MODELS\")\n",
    "plt.xlim(0, 100)\n",
    "plt.title(\"MODEL ACCURACY COMPARISON\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores={}\n",
    "accuracyscore = {}\n",
    "recallscore = {}\n",
    "precisionscore = {}\n",
    "fscore = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_noob_atheism = model_pipeline(RandomForestClassifier(random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"clf__n_estimators\": [100, 500, 1000, 2000],\n",
    "             \"clf__max_depth\": [3, 6, 12, 24, 48],\n",
    "             \"clf__criterion\": [\"gini\", \"entropy\"],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(RFC_noob_atheism, parameters, cv=2, n_jobs=-1, scoring=\"recall_macro\").fit(atheism_train[\"tweets\"], atheism_train[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"RFC\"] = gs_clf.best_score_\n",
    "scores[\"RFC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_tuned_atheism = Pipeline([('vect', CountVectorizer(tokenizer=lambda x: x, preprocessor= lambda x: x )\n",
    "                         ), ('tfidf', TfidfTransformer()\n",
    "                         ), ('clf', RandomForestClassifier(random_state=42, \n",
    "                                                       criterion = 'entropy', \n",
    "                                                       max_depth= 24, \n",
    "                                                       n_estimators= 100))]).fit(atheism_train[\"tweets\"], atheism_train[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = RFC_tuned_atheism.predict(atheism_val[\"tweets\"])\n",
    "ls = list(range(0,3))\n",
    "uniq, counts = np.unique(predict, return_counts=True)\n",
    "temp = [item for item in ls if item not in uniq]\n",
    "print(\"classes not predicted by model: {}\".format(temp))\n",
    "accuracyscore[\"RandomForestClassifier\"] = accuracy_score(atheism_val[\"labels\"], predict) \n",
    "recallscore[\"RandomForestClassifier\"] = recall_score(atheism_val[\"labels\"], predict, average=\"macro\")\n",
    "precisionscore[\"RandomForestClassifier\"] = precision_score(atheism_val[\"labels\"], predict, average=\"macro\")\n",
    "fscore[\"RandomForestClassifier\"] = f1_score(atheism_val[\"labels\"], predict, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SV_noob_atheism = model_pipeline(SVC(random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"clf__C\": [1,3,9,27],\n",
    "             \"clf__kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "             \"clf__degree\": [1,3,9,27],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(SV_noob_atheism, parameters, cv=2, n_jobs=-1, scoring=\"recall_macro\").fit(atheism_train[\"tweets\"], atheism_train[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"SV\"] = gs_clf.best_score_\n",
    "scores[\"SV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SV_tuned_atheism = Pipeline([('vect', CountVectorizer(tokenizer=lambda x: x, preprocessor= lambda x: x, token_pattern=None )\n",
    "                     ), ('tfidf', TfidfTransformer()\n",
    "                     ), ('clf', SVC(random_state=42, \n",
    "                                    C = 27, \n",
    "                                    degree= 3,\n",
    "                                    kernel=\"poly\"))]).fit(atheism_train[\"tweets\"], atheism_train[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = SV_tuned_atheism.predict(atheism_val[\"tweets\"])\n",
    "ls = list(range(0,3))\n",
    "uniq, counts = np.unique(predict, return_counts=True)\n",
    "temp = [item for item in ls if item not in uniq]\n",
    "print(\"classes not predicted by model: {}\".format(temp))\n",
    "accuracyscore[\"SupportVector\"] = accuracy_score( atheism_val[\"labels\"], predict) \n",
    "recallscore[\"SupportVector\"] = recall_score(atheism_val[\"labels\"], predict, average=\"macro\")\n",
    "precisionscore[\"SupportVector\"] = precision_score(atheism_val[\"labels\"], predict, average=\"macro\")\n",
    "fscore[\"SupportVector\"] = f1_score(atheism_val[\"labels\"], predict, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_noob_atheism = model_pipeline(SGDClassifier(loss='log_loss', random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"clf__alpha\": [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03],\n",
    "              \"clf__penalty\": [\"l2\", \"l1\", \"elasticnet\"],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(LR_noob_atheism, parameters, cv=2, n_jobs=-1, scoring=\"recall_macro\").fit(atheism_train[\"tweets\"], atheism_train[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"LR\"] = gs_clf.best_score_\n",
    "scores[\"LR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_tuned_atheism = Pipeline([('vect', CountVectorizer(tokenizer=lambda x: x, preprocessor= lambda x: x, token_pattern=None )\n",
    "                     ), ('tfidf', TfidfTransformer()\n",
    "                     ), ('clf', SGDClassifier(loss=\"log\",\n",
    "                                    random_state=42, \n",
    "                                    alpha = 0.0003, \n",
    "                                    penalty=\"l1\"\n",
    "                                             ))]).fit(atheism_train[\"tweets\"], atheism_train[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = LR_tuned_atheism.predict(atheism_val[\"tweets\"])\n",
    "ls = list(range(0,3))\n",
    "uniq, counts = np.unique(predict, return_counts=True)\n",
    "temp = [item for item in ls if item not in uniq]\n",
    "print(\"classes not predicted by model: {}\".format(temp))\n",
    "accuracyscore[\"LogisticRegression\"]  = accuracy_score( atheism_val[\"labels\"] , predict) \n",
    "recallscore[\"LogisticRegression\"]    = recall_score(   atheism_val[\"labels\"],  predict, average=\"macro\")\n",
    "precisionscore[\"LogisticRegression\"] = precision_score(atheism_val[\"labels\"],  predict, average=\"macro\")\n",
    "fscore[\"LogisticRegression\"]         = f1_score(       atheism_val[\"labels\"],  predict, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(15,8))\n",
    "ax[0,0].set_title(\"Accuracy score\")\n",
    "ax[0,1].set_title(\"F-score\")\n",
    "ax[1,0].set_title(\"Precision\")\n",
    "ax[1,1].set_title(\"Recall\")\n",
    "\n",
    "for i in accuracyscore.items():\n",
    "    ax[0,0].bar(i[0], i[1])\n",
    "    ax[0,0].set_ylim([0,0.8])\n",
    "\n",
    "for i in fscore.items():\n",
    "    ax[0,1].bar(i[0], i[1])\n",
    "    ax[0,1].set_ylim([0,0.8])\n",
    "    \n",
    "for i in precisionscore.items():\n",
    "    ax[1,0].bar(i[0], i[1])\n",
    "    ax[1,0].set_ylim([0,0.8])\n",
    "    \n",
    "for i in recallscore.items():\n",
    "    ax[1,1].bar(i[0], i[1])\n",
    "    ax[1,1].set_ylim([0,0.8])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we again see that the logistic regression is the best on all the metrics, thus it is our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final model on test data metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [0,1,2]\n",
    "model = LR_tuned_atheism\n",
    "predict = model.predict(atheism_test[\"tweets\"])\n",
    "print(\"LogisticRegression\" + \": \")\n",
    "uniq, counts = np.unique(predict, return_counts=True)\n",
    "temp = [item for item in ls if item not in set(uniq)]\n",
    "print(\"     - classes not predicted by model: \\t {}\".format(temp))\n",
    "print(\"     - accuracy score:\\t   {}\"  .format(accuracy_score( atheism_test[\"labels\"], predict)))\n",
    "print(\"     - recall score: \\t   {}\"   .format(recall_score(   atheism_test[\"labels\"], predict,  average=\"macro\")))\n",
    "print(\"     - precision score:\\t   {}\" .format(precision_score(atheism_test[\"labels\"], predict,  average=\"macro\")))\n",
    "print(\"     - f1 score:\\t   {}\"        .format(f1_score(       atheism_test[\"labels\"], predict,  average=\"macro\")))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our tokenizers on the final ML atheism model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores={}\n",
    "accuracyscore = {}\n",
    "recallscore = {}\n",
    "precisionscore = {}\n",
    "fscore = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideal tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_train = pd.concat([train_atheism, train_atheism_labels], axis=1)\n",
    "ideal_val = pd.concat([validation_atheism, validation_atheism_labels], axis=1)\n",
    "ideal_test = pd.concat([test_atheism,  test_atheism_labels],  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_train = ideal_train\n",
    "for idx, i in enumerate(copy_train[\"tweets\"]):\n",
    "    x = tokenize_ideal(i)\n",
    "    ideal_train.loc[idx, [\"tweets\"]] = x[0]\n",
    "\n",
    "copy_val = ideal_val\n",
    "for idx, i in enumerate(copy_val[\"tweets\"]):\n",
    "    x = tokenize_ideal(i)\n",
    "    ideal_val.loc[idx, [\"tweets\"]] = x[0]\n",
    "\n",
    "copy_test = ideal_test\n",
    "for idx, i in enumerate(copy_test[\"tweets\"]):\n",
    "    x = tokenize_ideal(i)\n",
    "    ideal_test.loc[idx, [\"tweets\"]] = x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = LR_tuned_atheism.predict(ideal_val[\"tweets\"])\n",
    "accuracyscore[\"ideal_tokenizer\"] = accuracy_score(ideal_val[\"labels\"], predict) \n",
    "recallscore[\"ideal_tokenizer\"] = recall_score(ideal_val[\"labels\"], predict, average=\"macro\")\n",
    "precisionscore[\"ideal_tokenizer\"] = precision_score(ideal_val[\"labels\"], predict, average=\"macro\")\n",
    "fscore[\"ideal_tokenizer\"] = f1_score(ideal_val[\"labels\"], predict, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize ekstra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ekstra_train = pd.concat([train_atheism, train_atheism_labels], axis=1)\n",
    "ekstra_val = pd.concat([validation_atheism, validation_atheism_labels], axis=1)\n",
    "ekstra_test = pd.concat([test_atheism, test_atheism_labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_train = ekstra_train\n",
    "for idx, i in enumerate(copy_train[\"tweets\"]):\n",
    "    x = tokenize_ekstra(i)\n",
    "    ekstra_train.loc[idx, [\"tweets\"]] = x[0]\n",
    "\n",
    "copy_val = ekstra_val\n",
    "for idx, i in enumerate(copy_val[\"tweets\"]):\n",
    "    x = tokenize_ekstra(i)\n",
    "    ekstra_val.loc[idx, [\"tweets\"]] = x[0]\n",
    "\n",
    "copy_test = ekstra_test\n",
    "for idx, i in enumerate(copy_test[\"tweets\"]):\n",
    "    x = tokenize_ekstra(i)\n",
    "    ekstra_test.loc[idx, [\"tweets\"]] = x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = LR_tuned_atheism.predict(ekstra_val[\"tweets\"])\n",
    "accuracyscore[\"ekstra_tokenizer\"] = accuracy_score(ekstra_val[\"labels\"], predict) \n",
    "recallscore[\"ekstra_tokenizer\"] = recall_score(ekstra_val[\"labels\"], predict, average=\"macro\")\n",
    "precisionscore[\"ekstra_tokenizer\"] = precision_score(ekstra_val[\"labels\"], predict, average=\"macro\")\n",
    "fscore[\"ekstra_tokenizer\"] = f1_score(ekstra_val[\"labels\"], predict, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_tokenizer(line):\n",
    "    tok = nltk.tokenize.TreebankWordTokenizer()\n",
    "    new = \"\"\n",
    "    for t in tok.tokenize(line):\n",
    "        new = new + \" \" +t\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_train = pd.concat([train_atheism, train_atheism_labels], axis=1)\n",
    "gold_val   = pd.concat([validation_atheism, validation_atheism_labels], axis=1)\n",
    "gold_test  = pd.concat([test_atheism,  test_atheism_labels],  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_train = gold_train\n",
    "for idx, i in enumerate(copy_train[\"tweets\"]):\n",
    "    x = gold_tokenizer(i)\n",
    "    gold_train.loc[idx, [\"tweets\"]] = x\n",
    "\n",
    "copy_val = gold_val\n",
    "for idx, i in enumerate(copy_val[\"tweets\"]):\n",
    "    x = gold_tokenizer(i)\n",
    "    gold_val.loc[idx, [\"tweets\"]] = x\n",
    "\n",
    "copy_test = gold_test\n",
    "for idx, i in enumerate(copy_test[\"tweets\"]):\n",
    "    x = gold_tokenizer(i)\n",
    "    gold_test.loc[idx, [\"tweets\"]] = x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = LR_tuned_atheism.predict(gold_val[\"tweets\"])\n",
    "accuracyscore[\"nltk_tokenizer\"]  = accuracy_score( gold_val[\"labels\"] , predict) \n",
    "recallscore[\"nltk_tokenizer\"]    = recall_score(   gold_val[\"labels\"],  predict, average=\"macro\")\n",
    "precisionscore[\"nltk_tokenizer\"] = precision_score(gold_val[\"labels\"],  predict, average=\"macro\")\n",
    "fscore[\"nltk_tokenizer\"]         = f1_score(       gold_val[\"labels\"],  predict, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no tokenizer\n",
    "To compare the tokenizers with the scenario where we do not use a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = pd.concat([train_atheism, train_atheism_labels], axis=1)\n",
    "raw_val   = pd.concat([validation_atheism, validation_atheism_labels], axis=1)\n",
    "raw_test  = pd.concat([test_atheism,  test_atheism_labels],  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = LR_tuned_atheism.predict(raw_val[\"tweets\"])\n",
    "accuracyscore[\"raw\"]  = accuracy_score( raw_val[\"labels\"] , predict) \n",
    "recallscore[\"raw\"]    = recall_score(   raw_val[\"labels\"],  predict, average=\"macro\")\n",
    "precisionscore[\"raw\"] = precision_score(raw_val[\"labels\"],  predict, average=\"macro\")\n",
    "fscore[\"raw\"]         = f1_score(       raw_val[\"labels\"],  predict, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluatin of tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all the tokenizers produce different results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_val[\"tweets\"].iloc[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ekstra_val[\"tweets\"].iloc[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_val[\"tweets\"].iloc[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_val[\"tweets\"].iloc[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(15,8))\n",
    "ax[0,0].set_title(\"Accuracy score\")\n",
    "ax[0,1].set_title(\"F-score\")\n",
    "ax[1,0].set_title(\"Precision\")\n",
    "ax[1,1].set_title(\"Recall\")\n",
    "\n",
    "for i in accuracyscore.items():\n",
    "    ax[0,0].bar(i[0], i[1])\n",
    "    ax[0,0].set_ylim([0,0.9])\n",
    "\n",
    "for i in fscore.items():\n",
    "    ax[0,1].bar(i[0], i[1])\n",
    "    ax[0,1].set_ylim([0,0.9])\n",
    "    \n",
    "for i in precisionscore.items():\n",
    "    ax[1,0].bar(i[0], i[1])\n",
    "    ax[1,0].set_ylim([0,0.9])\n",
    "    \n",
    "for i in recallscore.items():\n",
    "    ax[1,1].bar(i[0], i[1])\n",
    "    ax[1,1].set_ylim([0,0.9])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the tokenizer, which we have identified to be the best and most appropriate for this project, perform relatively equal to the baseline tokenizer. Oddly enough they perform as good as the raw data, which is weird. If we were to use the tokenizer implemented in CountVectorizer we would see an increase in our metrics, thus if we were to do this project for real and the rule of using our own tokenizer did not exist, then we could achieve higher accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For futher studies much more data would be necessary to do word embedding and in general improve our models. Especially more data in the stance-files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#### Binary classification for Irony:\n",
    "Final model: LogisticRegression\n",
    "     - accuracy score:       0.5535714285714286\n",
    "     - recall score:        0.6688102893890675\n",
    "     - precision score:       0.45714285714285713\n",
    "     - f1 score:       0.5430809399477806\n",
    "Overall the highest accuracy score of all of the tuned models were equally bad at classifying whether a given sentence is ironic. ince the group members also performed badly at classifying irony, we can not expect the model to outperform us massively, since it lacks world knowledge and general experience and emotional aspect. \n",
    "Combining an intention model and an irony model could possibly increase the performance of the irony model.\n",
    "\n",
    "\n",
    "#### Multi-class classification for Atheism:\n",
    "Final model: LogisticRegression\n",
    "     - classes not predicted by model:      []\n",
    "     - accuracy score:       0.6409090909090909\n",
    "     - recall score:        0.3354166666666667\n",
    "     - precision score:       0.3465909090909091\n",
    "     - f1 score:       0.33073166826769135\n",
    "The model for classifying the stance in the dataset Atheism performed best out of the bunch. In the end the LogisticRegression model was chosen, as it included all labels, opposite some of the other models, and had the overall highest scores.\n",
    "The distribution looked fairly linear and we had to use the macro-average recall, because the data was unbalanced - which was likely also the cause of some of the models leaving out some of the classes when predicting.\n",
    "\n",
    "\n",
    "#### Multi-class classification for all stances:\n",
    "Final model: KNN\n",
    "     - classes not predicted by model:      []\n",
    "     - accuracy score:       0.21857485988791034\n",
    "     - recall score:        0.1293985722138177\n",
    "     - precision score:       0.1290124308772997\n",
    "     - f1 score:       0.12514616152456073\n",
    "The KNN was seen as the best models, since it only excluded one classification out of the total 15 on the training data. Given the difficulty of the task with this many different classes the accuracy of the KNN is surprisingly high - better than random, yet still bad. Safe to say is that little amount of training data for some classes (imbalance) and a lot of labels to predict from is not promissing.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating tokenizer\n",
    "Our final developed tokenizer performed relatively equal to the baseline tokenizer. If doing this project for real higher accuracies could be achieved if we were to use the tokenizer implemented in CountVectorizer, as this would increase our metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "62a3362af9fb780a0eb03402584da787fc1a1b0aa4d8f94c680e96f1c63d9193"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
